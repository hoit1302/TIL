# ê°•í™” í•™ìŠµ

Reinforcement learning: Learning how to take actions in order to maximize reward via trial and errors

ì˜ˆì‹œ

- ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ê²Œì„ì„ ì´ê¸°ê¸° ìœ„í•´ ì–´ë–»ê²Œ ë°”ë‘‘ëŒì„ ë‘˜ì§€ë¥¼ í•™ìŠµ

- atari gameì—ì„œ ë²½ëŒì„ ìµœëŒ€í•œ ë§ì´ ê¹¨ê¸° ìœ„í•´ í•™ìŠµ

- ë¡œë´‡ íŒ” ì œì–´

- ê¸¸ì°¾ê¸°ì—ì„œ ìµœë‹¨ê±°ë¦¬ë¡œ ê°€ê¸° ìœ„í•œ ì›€ì§ì„

## Markov Decision Process (MDP)

ë§ˆë¥´ì½”í”„ë¼ê³  ì½ëŠ”ë‹¤.

- Set of states ğ‘†, Set of actions ğ´

- ê° timeë§ˆë‹¤, agent observes state ğ‘ \_ğ‘¡ âˆˆ ğ‘†, then chooses action ğ‘\_ğ‘¡ âˆˆ ğ´ , then receives reward ğ‘Ÿ\_ğ‘¡, and state changes to ğ‘ \_ğ‘¡+1

- Markov assumption(ì¶”ì •):
  
  - ë³´í†µ ì´ì „ state, ê·¸ ì´ì „ state, ë” ì´ì „ state ë“¤ì´ actionì— ì˜í–¥ì„ ì¤„í…ë°
  
  - ë‹¤ìŒ stateê°€ ì˜¤ë¡œì§„ í˜„ì¬ stateì™€ í˜„ì¬ ì·¨í•œ actionì—ë§Œ dependentí•˜ë„ë¡ ê°€ì •í•¨.
  
  - ìˆ˜í•™ì´ ì‰¬ì›Œì§
  
  - ![image](https://user-images.githubusercontent.com/68107000/172507212-f48a2cf3-e924-4f2b-bce5-730dfe61e930.png)

- Also assume reward Markov:
  
  - í˜„ì¬ ë°›ì„ ìˆ˜ ìˆëŠ” rewardëŠ” í˜„ì¬ ìƒí™©ì—ì„œ actionì„ ì·¨í–ˆì„ ë•Œì—ë§Œ ì˜ì¡´
  
  - ![image](https://user-images.githubusercontent.com/68107000/172507119-67227317-9c1f-44ee-b425-825cdfe6305e.png)

- The task: learn a policy ğœ‹: ğ‘† â†’ ğ´ for choosing actions that maximizes:
  
  - policy ğœ‹: ëª¨ë“  stateì—ì„œ ì–´ë–¤ actionì„ ì·¨í• ê±´ê°€ë¥¼ define í•´ì£¼ëŠ” ê²ƒ
  
  - s_0ì—ì„œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ê°€ëŠ¥ì„±ì— ëŒ€í•´ì„œ expected rewardë¥¼ maximize í•˜ëŠ” ì¼ë ¨ì˜ actionë“¤ ì°¾ê¸° â†’ optimalí•œ policy ì°¾ê¸°
  
  - ![image](https://user-images.githubusercontent.com/68107000/172507143-60b6187c-ca1c-4e4d-82af-30a4e3de4b12.png)

### MDP: Example, Atari game

- ëª©í‘œ: ê°€ì¥ ë†’ì€ ì ìˆ˜ ì–»ê¸°

- state: í˜„ì¬ì˜ raw pixel inputs

- action: ì»¨íŠ¸ë¡¤ (ìƒ,í•˜,ì¢Œ,ìš°)

- reward: ê° time stepë§ˆë‹¤ ì ìˆ˜ë¥¼ ì–»ê±°ë‚˜ ìƒìŒ

## Reinforcement Learning

**MDPì„ í•´ê²°í•˜ëŠ” ëª¨ë“  ì•Œê³ ë¦¬ì¦˜**

*rewardë¥¼ maximize í•´ì£¼ëŠ” ë°©í–¥ìœ¼ë¡œ ì¼ë ¨ì˜ action ì°¾ê¸°*

![image](https://user-images.githubusercontent.com/68107000/172508199-065eb2d4-295b-470e-9ffd-913a8eec7a6f.png)

ê°ë§ˆ: discout factor, í˜„ì¬ ë‹¹ì¥ actionì„ ì·¨í•´ ì–»ëŠ” rewardê°€ ë‚˜ì¤‘ì— actionì„ ì·¨í•´ ì–»ì€ rewardë³´ë‹¤ ì¡°ê¸ˆ ë” ì¤‘ìš”í•¨

Î³_0 ëŠ” í˜„ì¬ ê°€ì¹˜ë¡œ, ê°€ì¥ ê°’ì´ í¬ë‹¤.

Î³_0, Î³_1, Î³_2 ê°€ ì¶•ì ë˜ì–´ maximizeí•´ì£¼ëŠ” ë°©í–¥ìœ¼ë¡œ!

## Value Function for Each Policy

- \<s, a\>: sì— ìˆìœ¼ë©´ ì–´ë–¤ actionì„ ì·¨í•´ë¼ ì™€ ê°™ì€ exampleì´ ìˆì–´ì•¼í•˜ëŠ”ë°, **ì—†ìŒ**.

- \<\<s, a\>, r\>: <í˜„ì¬ s stateì— ìˆê³  a actionì„ ì·¨í•˜ë©´, ë³´ìƒì´ r ì„>ì„ ì•Œê³  ìˆë‹¤.

ë”°ë¼ì„œ **value function**ìœ¼ë¡œ ì°¾ëŠ”ë‹¤.

starting pointì—ì„œ ì‹œì‘í•˜ëŠ” policy ğœ‹ê°€ ì£¼ì–´ì¡Œì„ ë•Œ value functionì€ ì´ë ‡ê²Œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/68107000/172509458-87c39084-8f5b-4e3c-a9df-4498acc7e62f.png)

optimal policy ğœ‹\*ë¥¼ ì›í•œë‹¤.

![image](https://user-images.githubusercontent.com/68107000/172509508-ffd49560-7d33-451e-a26e-7b49e56ed7e2.png)

MDPì—ì„œ optimal policyëŠ” ì¡´ì¬í•œë‹¤.

ë§Œì•½ optimalì„ ë”°ë¥¼ ë•Œì˜ value function V*(s)ì´ ìˆê³ , ë‹¤ìŒ state P(s\_t+1|s_t, a) ë¥¼ ì•Œë©´ ğœ‹*(s)ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

### value function example:  ![image](https://user-images.githubusercontent.com/68107000/172509881-23919aaf-8a66-4859-8ca1-22aa743d5b1b.png) values

![image](https://user-images.githubusercontent.com/68107000/172510917-dd344369-9a44-4171-94d4-e3758cf607c4.png)

### value function example: ![image](https://user-images.githubusercontent.com/68107000/172510056-e58ee402-c952-4a7a-9221-335e2f5350bc.png) values

![image](https://user-images.githubusercontent.com/68107000/172510954-ebe26c50-c8a4-4b7c-a1b7-51f481f69972.png)

## Recursive Definition for <img title="" src="https://user-images.githubusercontent.com/68107000/172510056-e58ee402-c952-4a7a-9221-335e2f5350bc.png" alt="image" data-align="inline">

![image](https://user-images.githubusercontent.com/68107000/172511386-bbd20351-1024-4ed5-94e9-ed26fdcc119a.png)

s2ì—ì„œ ì‹œì‘í•˜ëŠ” optimal ê°€ì¹˜ í•¨ìˆ˜ë¥¼ í•œ ê°œì˜ termìœ¼ë¡œ ì •ë¦¬í–ˆë‹¤.

![image](https://user-images.githubusercontent.com/68107000/172511464-d588c8cc-f0f8-42bf-bb6a-5be75dee7740.png)

í˜„ì¬ stateì— ìˆì„ ë•Œ optimalí•œ policyì— ì˜í•´ ê²°ì •ëœ action & reward

optimal policyì— ì˜í•´ í˜„ì¬ state s ì—ì„œ s'ìœ¼ë¡œ ê°€ê³ , s'ì— ìˆì„ ë•Œì˜ ê°€ì¹˜í•¨ìˆ˜
