# 강화 학습

Reinforcement learning: Learning how to take actions in order to maximize reward via trial and errors

예시

- 시행착오를 통해 게임을 이기기 위해 어떻게 바둑돌을 둘지를 학습

- atari game에서 벽돌을 최대한 많이 깨기 위해 학습

- 로봇 팔 제어

- 길찾기에서 최단거리로 가기 위한 움직임

## Markov Decision Process (MDP)

마르코프라고 읽는다.

- Set of states 𝑆, Set of actions 𝐴

- 각 time마다, agent observes state 𝑠\_𝑡 ∈ 𝑆, then chooses action 𝑎\_𝑡 ∈ 𝐴 , then receives reward 𝑟\_𝑡, and state changes to 𝑠\_𝑡+1

- Markov assumption(추정):
  
  - 보통 이전 state, 그 이전 state, 더 이전 state 들이 action에 영향을 줄텐데
  
  - 다음 state가 오로진 현재 state와 현재 취한 action에만 dependent하도록 가정함.
  
  - 수학이 쉬워짐
  
  - ![image](https://user-images.githubusercontent.com/68107000/172507212-f48a2cf3-e924-4f2b-bce5-730dfe61e930.png)

- Also assume reward Markov:
  
  - 현재 받을 수 있는 reward는 현재 상황에서 action을 취했을 때에만 의존
  
  - ![image](https://user-images.githubusercontent.com/68107000/172507119-67227317-9c1f-44ee-b425-825cdfe6305e.png)

- The task: learn a policy 𝜋: 𝑆 → 𝐴 for choosing actions that maximizes:
  
  - policy 𝜋: 모든 state에서 어떤 action을 취할건가를 define 해주는 것
  
  - s_0에서 시작하는 모든 가능성에 대해서 expected reward를 maximize 하는 일련의 action들 찾기 → optimal한 policy 찾기
  
  - ![image](https://user-images.githubusercontent.com/68107000/172507143-60b6187c-ca1c-4e4d-82af-30a4e3de4b12.png)

### MDP: Example, Atari game

- 목표: 가장 높은 점수 얻기

- state: 현재의 raw pixel inputs

- action: 컨트롤 (상,하,좌,우)

- reward: 각 time step마다 점수를 얻거나 잃음

## Reinforcement Learning

**MDP을 해결하는 모든 알고리즘**

*reward를 maximize 해주는 방향으로 일련의 action 찾기*

![image](https://user-images.githubusercontent.com/68107000/172508199-065eb2d4-295b-470e-9ffd-913a8eec7a6f.png)

감마: discout factor, 현재 당장 action을 취해 얻는 reward가 나중에 action을 취해 얻은 reward보다 조금 더 중요함

γ_0 는 현재 가치로, 가장 값이 크다.

γ_0, γ_1, γ_2 가 축적되어 maximize해주는 방향으로!

## Value Function for Each Policy

- \<s, a\>: s에 있으면 어떤 action을 취해라 와 같은 example이 있어야하는데, **없음**.

- \<\<s, a\>, r\>: <현재 s state에 있고 a action을 취하면, 보상이 r 임>을 알고 있다.

따라서 **value function**으로 찾는다.

starting point에서 시작하는 policy 𝜋가 주어졌을 때 value function은 이렇게 정의할 수 있다.

![image](https://user-images.githubusercontent.com/68107000/172509458-87c39084-8f5b-4e3c-a9df-4498acc7e62f.png)

optimal policy 𝜋\*를 원한다.

![image](https://user-images.githubusercontent.com/68107000/172509508-ffd49560-7d33-451e-a26e-7b49e56ed7e2.png)

MDP에서 optimal policy는 존재한다.

만약 optimal을 따를 때의 value function V*(s)이 있고, 다음 state P(s\_t+1|s_t, a) 를 알면 𝜋*(s)를 계산할 수 있다.

### value function example:  ![image](https://user-images.githubusercontent.com/68107000/172509881-23919aaf-8a66-4859-8ca1-22aa743d5b1b.png) values

![image](https://user-images.githubusercontent.com/68107000/172510917-dd344369-9a44-4171-94d4-e3758cf607c4.png)

### value function example: ![image](https://user-images.githubusercontent.com/68107000/172510056-e58ee402-c952-4a7a-9221-335e2f5350bc.png) values

![image](https://user-images.githubusercontent.com/68107000/172510954-ebe26c50-c8a4-4b7c-a1b7-51f481f69972.png)

## Recursive Definition for <img title="" src="https://user-images.githubusercontent.com/68107000/172510056-e58ee402-c952-4a7a-9221-335e2f5350bc.png" alt="image" data-align="inline">

![image](https://user-images.githubusercontent.com/68107000/172511386-bbd20351-1024-4ed5-94e9-ed26fdcc119a.png)

s2에서 시작하는 optimal 가치 함수를 한 개의 term으로 정리했다.

![image](https://user-images.githubusercontent.com/68107000/172511464-d588c8cc-f0f8-42bf-bb6a-5be75dee7740.png)

현재 state에 있을 때 optimal한 policy에 의해 결정된 action & reward

optimal policy에 의해 현재 state s 에서 s'으로 가고, s'에 있을 때의 가치함수
