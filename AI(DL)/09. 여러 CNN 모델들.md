# Various CNN Models

## AlexNet

- Named after "Alex" Krizhevsky

- Won ILSVRC 2012 when he was a Ph.D. student under the supervision of Prof. Hinton at University of Toronto

- Btw, there is a short behind story on this regarding **hw/sw co-operation**

## Architecture

![image](https://user-images.githubusercontent.com/68107000/164884456-0a49d161-9fc1-48a6-afc1-91508b853ea1.png)

```
CONV1: {
    Input: 227x227x3 (padding: 224+3, channel: rgb 3)
    Filters: 96(개) 11x11x3 at stride 4
    Output: 55x55x96
}
MAXPOOL1: {
    Input: 55x55x96
    3x3 max pooling with stride 2
    Output: 27x27x96 (27 = (55 - 3) / 2 + 1)
}
NORM1
CONV2: 256 5x5 filters at stride 1, pad 2
MAXPOOL2: 3x3 windows at stride 2
NORM2
CONV3: 384 3x3 filters at stride 1, pad 1
CONV4: 384 3x3 filters at stride 1, pad 1
CONV5: 256 5x5 f ilters at stride 1, pad 2
MAXPOOL3: 3x3 windows at stride 2
FC6: 4096 neurons (weight 수는: 7^2 * 256 * 4096)
FC7: 4096 neurons (weight 수는: 4096 * 4096)
FC8: 1000 neurons (equal to the number of classes), (weight 수는: 4096 * 1000)
```

Total number of weights: **62M**

### Some more details

- First use of ReLU

- Used Local Response Normalization (LRN)

- Data augmentation

- Dropout 0.5

## VGG

### key concept

**small filters, deeper layers**

- CONV: 오직 3x3 filters at stride 1, pad 1

- MAXPOOL: 오직 2x2 windows at stride 2

- **Stack of three 3x3 CONV layers** has the same effective receptive field as one 7x7 CONV layer

- But **3개의 CONV layer** has more non-linearities and fewer parameters
  
  - 3 \* 3 \* 3 vs. 7 \* 7

![image](https://user-images.githubusercontent.com/68107000/165003113-8f8fb657-b6d7-4c7e-8a93-1cf522fb1aca.png)

- VGG 16의 정보

- VGG16은 총 weight 개수가 138M

- 2X2 maxpooling을 하니 feature map의 height와 width가 반절로

- input의 channel과 conv filter의 채널이 같아야 함.

- conv의 filter 수가 (첫번째 수) output의 채널수가 됨 (feature maps의 3번째 수)

## GoogLeNet

deeper layers(bottleneck block) with computational efficiency (계산량 줄이면서) with "**inception module**"

### Naive Inception Module

![image](https://user-images.githubusercontent.com/68107000/165031078-be7af5a5-1ff0-49fb-8dbb-82b0e37d2396.png)

- Apply parallel filters on the input from previous layer (병렬적으로 같은 input에 적용시킴)
  
  - Multiple filter sizes (multiple receptive fields) + Pooling

- Concatenate all filter outputs channel-wise

그런데 이 방법은 연산 효율이 매우 안좋다.

![image](https://user-images.githubusercontent.com/68107000/165031094-e281dcc2-bc58-48b6-a883-0ef5c94d7549.png)

→ **Problem**: huge computational complexity

### Solution: 1x1 convolution

![image](https://user-images.githubusercontent.com/68107000/165032108-67f639e5-0486-4c8b-9ca9-6d53b8e1e3d9.png)

- Preserves spatial dimensions, reduces depth, 동일한 dimension에 channel size 줄임.

- Projects depth to lower dimension (64 → 32)

![image](https://user-images.githubusercontent.com/68107000/165036534-f4320565-eb46-4bc7-8d3b-279c9a25a68f.png)

- 채널 줄이기 위한 것을 1x1 CONV "**bottleneck**" layer라고 함.

- bottlenect block을 쓰면 정보의 손실이 생긴다. accuracy가 줄어들 여지가 있다. 대신 연산량이 대폭 줄어들어 network를 키울 여지가 생긴다. 즉 accuracy를 높일 수 있다.

- 같은 연산량 대비 더 높은 accuary를 얻을 수 있는 효과를 얻게 되었다.

### Architecture

![image](https://user-images.githubusercontent.com/68107000/165031282-e0e6a8f9-e212-42e2-b080-26df3410f0f3.png)

- Total number of weights: **5M** (12x less than AlexNet-62M, 27x less than VGG16-138M)

- **classfier**: 3FC 에서 1FC로 줄여도 충분하다는 경험들이 있었음.

- **Auxiliary Classifier**: 맨 뒤에서 앞 layer로 갈수록 gradient 값이 줄어들어서 앞 layer는 거의 training이 되지 않는 문제가 있음. gradient를 injection해줘서 작아진 gradient에 보상해줘 training되도록 함.

---

## ResNet

ImageNet Competition에서 깊이의 획기적인 revolution을 보여주면서 우승한 네트워크

<br\>

Q. 기존 모델의 layer를 높이면 정확도가 올라가는 거 아닌가요?

![image](https://user-images.githubusercontent.com/68107000/165072373-d95b38a6-a9ea-4aef-be57-3a188db8a36d.png)

A. layer가 깊다고 해서 반드시 성능이 좋아지는 게 아니다. representation power가 높은 건 사실이지만 모델이 깊어질수록 성능이 나빠졌다. 과적합 때문은 아니고 optimize (쉽게 말해 학습이) 어렵기 때문이다. 학습이 어려운 이유 중 하나가 **gradient vanishing problem**이다.

**gradient vanishing problem**: With small weights, consecutive derivatives make gradient vanish at lower layers. 1보다 작은 수를 계속해서 곱하다보니 값이 매우매우 작아진다.

**gradient exploding problem**: With large weights, consecutive derivatives make gradient explode at lower layers. 숫자가 표현할 수 없을 정도로 커짐 (nan) 이 때 loss 값도 엄청 커진다.

이때 ReLU가 gradient vanishing problem을 완화시킬 수 있다.

![image](https://user-images.githubusercontent.com/68107000/165072531-737c5554-fa42-421e-9950-ed44745c364a.png)

Sigmoid: sigmoid는 0\~1 사이의 값을 갖는다. (hyperbolic tanh: -1\~1) activation (=feature)를 0과 1사이의 값으로 밀어넣어 gradient vanishing 문제를 가속시킨다.

ReLU: 0 이상의 어떤 값이나 될 수 있다. gradient가 사라지는 일은 거의 없다. 그래서 ReLU가 가장 많이 쓰인다!!!

### key concept

original paper의 설명:

- 극단적인 예시: 얕은 모델과 같은 결과를 내지만 더 깊은 모델을 만들고 싶어서 indentity mapping을 사용했음.

- identity block은 어떻게 만들지? 
  
  - H(x) → x 하도록 train이 어려워서 (x가 변함), F(x) = H(x) - x → 0 (고정값) 으로 더 쉽게 학습
  
  - 이 때 H(x) - x를 residual(잔여물)이라 부름.

후속 논문의 설명:

- residual block이 vanishing gradient problem을 막는다고 설명함.

- CONV layer를 통과하면서 작은 수와 곱셈하여 gradients of CONV layers가 점점 작아진다. 이 때 어떠한 곱셈도 하지 않은 보존된 gradient와 더해 크기를 키운다.

### Architecture (ResNet-34)

![image](https://user-images.githubusercontent.com/68107000/165049107-8180a28d-c67e-405b-a253-bb0f31b86724.png)

- 각 하나의 residual block에는 2개의 3x3 conv layer와 하나의 identity mapping이 존재한다.

- 주기적으로 필터의 개수를 2배로 늘리고 대신에 feature size를 2 stride로 downsampling 한다.
  
  - filter 개수 2배 → 연산량은 2배
  
  - stride 2 → 연산량은 ¼배
  
  - 따라서 총 연산량은 ½배로, 줄어든다.

- output에는 GoogLeNet과 같이 Avgpool + 1 FC layer

![image](https://user-images.githubusercontent.com/68107000/165051157-8491423a-fbca-4300-a7b2-8229a9672b16.png)

- 깊어지면 GoogLeNet처럼 Bottleneck layer가 존재해 efficiency 효율을 높인다.
- FLOPS (Floating Operations), 1MAC = 2FLOPS

## MobileNet

### motivation

A standard convolution has the effect of **filtering** features and **combining** features in order to produce a new representation.

filtering 하는 부분과 combining 하는 부분이 엮여있다.

이걸 분리시키면 computational cost를 낮출 수 있다.

### key idea

전체 conv layer를 2개로 나눴다.

Depthwise convolution + pointwise convolution → Depthwise separable convolution

- Depthwise convolution: 한 채널씩 convolution 한다.

- pointwise convolution: 1x1 conv으로 채널의 dimension을 1로 축소시킨다.

**특징**

- standard conv는 spatial 방향과 channel 방향을 동시에 고려한다.

- Depthwise separable conv는 depth 끼리만 conv만 한 다음 더해준다.

- 정보의 손실이 있어 acc 소폭 작아지지만 computational cost가 대폭 작아진다.

- 그래서 가벼운 휴대폰이나 IoT에 사용한다. 읽는 것보다 얻는 것이 크다.

### architecture

![image](https://user-images.githubusercontent.com/68107000/164885229-dce1fe27-6605-46c3-a840-1e5fb7c614d7.png)

모든 conv 연산은 depthwise(dw) convolution 으로 이루어져있다.

standard convolution은 bottleneck을 위해 사용한다.

### computational complexity

![image](https://user-images.githubusercontent.com/68107000/164885134-abdc2ca6-cb57-4155-b01d-2b70a3348549.png)

Optimized for **on-device inference** due to the reduction of computational complexity at the cost of little accuracy loss

MobileNet은 acc를 비슷하게 유지하면서 GoogleNet보다 1/2, VGG 16보다 1/20배 연산이 줄였다.

## YOLO

### motivation

앞선 네트워크는 classification을 위해서였다면 YOLO는 object detection을 위해 사용한다.

### previous approach: fast RCNN

- RoI(region of Interest) projection: object가 있을 가능성이 높은 부분을 찾고 그 이미지를 CONV 네트워크로 수행한다.

- 이미지 1장에서 ROI가 여러 개인데 각 ROI에 대해서 detection을 수행하므로 매우 느리다.

- 실시간으로 하는 게 중요한데, 너무 무겁기 때문에 실시간이 불가능했다.

### key idea

- 이미지 하나 당 CONV 네트워크를 한번만 수행한다.

- RoI 자체, bounding box를 훈련을 통해 얻는다.

- image를 S×S grid로 나누고 그리드 안에서 bounding box를 regression 해준 뒤 (어떤bounding box가 confidence가 높은가에 대한 결과와 class probability map-영역으로 분할도 결과로 나옴)

- 물체의 중심이 그리드 셀에 들어가면 해당 그리드 셀이 해당 물체를 감지한다.

- train them by using loss function for box offsets

### architecture

하나의 이미지에 하나의 CNN만 넣기 때문에 빠르다.

YOLO는 3가지 scale에 대해서 detection하기 때문에 acc가 높다.

획기적으로 적은 시간이 걸린다. 대신 정확도/성능은 조금 낮다.

실시간성이 보장되어야한다면 YOLO를 쓰면 좋다. 그렇지않다면 다른 네트워크가 accuracy 측면에서 더 좋다.

### limitation

- 각 그리드 cell은 최대 2개 bounding box를 예측할 수 있고, 1개의 class를 가진다.
  
  - 한 그리드 안에 여러 object가 있는 경우에는 제한된다.

- dataset으로 training하기에 dataset에 없는 new bounding box 크기나 unusual aspect ratios or configuration 같은 경우에는 잘 동작하지 않는다.
