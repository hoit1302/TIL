# Natural Language Processing

## Natural Language Processing

NLP는 컴퓨터로 사람 언어를 사용하는 것이다. 

human language는 ambiguous 하다.

C나 Python같은 Formal L은 unambiguous하다.

기계 번역, 감정 분류, 스팸 필터링, 챗봇 등 commercial product에서도 안정적으로 서비스되고 있다.

이러한 많은 NLP application들은 **language model**을 기반으로 하고 있다.

## Language Model

연속된 단어의 확률 분포를 assign하면서 언어를 모델링한다. (simulate한다)

예를 들면, 주어진 일련의 단어들로 다음 단어를 예측하는 것이다.

![image](https://user-images.githubusercontent.com/68107000/171751406-8814a533-fb4e-456c-aeba-a98c633578bf.png)

condition에서, w_5에 무엇이 올지에 대한 확률

![image](https://user-images.githubusercontent.com/68107000/171751721-d21a3b6f-c2cf-4a29-9ce4-f38ec8d3cd8e.png)

그림은 unrolled RNN이고, 이것이 word level Language model의 역할을 하고 있다. 

## How to Represent Words in Numbers?

학습시키기 위해서는 단어를 숫자로 표현해야한다. 어떻게 표현할 수 있을까?

### One-hot vector (Sparse representation)

![image](https://user-images.githubusercontent.com/68107000/171752523-f69ea914-e4fe-4dbd-a0cb-5339020e5722.png)

영어의 모든 단어를 describe한 vocabulary가 있다. 

사이즈가 10000이라 했을 때, 그 중 한 단어에 대응되도록 해당 번째 idx의 값만 1로 하고 나머지 값은 0으로 지정한다.

1이 희소하다고 해서 sparse representation이라고 한다.

이렇게 표현하면 vector size가 너무 크고 queen, woman이 연관되어 있지만 표현할 수 없게 된다.

### Embedding vector (Dense representation)

![image](https://user-images.githubusercontent.com/68107000/171752534-caa24ffe-631b-40ed-b021-380f43fcb96c.png)

벡터의 각 원소는 단어가 아닌 특징을 의미한다. 특징만을 벡터로 표현하기 때문에 벡터 사이즈가 현저히 줄어든다.

Queen과 Woman은 여성이기 때문에 1쪽으로 값이 치우쳐진 것을 볼 수 있다. 연관 관계를 잘 표현할 수 있게 되었다.

값이 있는 idx가 많아 dense representation이라고 부른다.

## Embedding Table

![](C:\Users\Jueun\AppData\Roaming\marktext\images\2022-06-03-08-10-04-image.png)

embedding table에 Apple을 나타내는 one-hot vector를 곱해 embedding vector를 얻을 수 있다. 조금 더 쉽게 설명하면, embedding table에서 456번째 column을 추출했다고 볼 수 있는데, 이렇게 특정 column을 가져오는 것을 **embedding lookup**이라고도 한다.

그렇다면 embedding table는 어떻게 만들까? 기계학습으로 training 시켜 얻는다!

embedding vector는 feature vector를 추출했다고 볼 수 있다.

## Word2Vec: Continuous Bag of Words

Key Idea: **Distributional Hypothesis**, 같이 반복해서 나온 단어일수록 유사도가 높다.

<img src="https://user-images.githubusercontent.com/68107000/171753661-db5cb5ae-bc2d-4621-b67e-bfe1ee445d27.png" title="" alt="image" data-align="center">

window size가 2라면 중심단어 양 옆으로 2개의 단어 (총 4개)들로부터 중심 단어를 지우고 supervised learning으로 학습키는 방법이다.



<p align="center"><img src="https://user-images.githubusercontent.com/68107000/171753440-dec31bcf-ca96-44a5-88bb-c4e3b2af3aa6.png"></p>

embedding table을 만들기 위해 위와 같은 네트워크를 꾸몄다.

일종의 FC 네트워크라고 할 수 있다.

V_fat, V_cat, V_on, V_the 의 평균을 취한 것이 embedding vector M이 된다.

one-hot vector에서 embedding vector를 구하는 과정에서 weight matrix를 곱하게 된다. 이 때의 weight matrix는 v X M의 모양이다.

embedding vector에서 output인 one-hot vector를 계산할 때 weight matrix를 곱한다. 이 때의 weight matrix는 M X v의 모양이다.

training된 2개의 weight matrix 중 하나를 선택하거나 2개를 average하여 embedding table로 사용하게 된다.

## Word2Vec: Skip-Grams

sat 하나의 단어로 주변 4개를 유추할 수 있도록 학습시킨다. CBoW의 방법과 반대이다.

![image](https://user-images.githubusercontent.com/68107000/171847166-474bea6e-d0b7-4c36-bdf4-b8d50cdbae9e.png)

input layer에서 projection layer로 계산할 때 weight matrix W1가 사용된다.

projection layer에서 output layer 계산할 때 weight matrix W2가 사용된다.

output layer는 softmax를 거쳐 output으로 나오기 되고 ground truth와 비교하여 update하게 된다.

단어(vocab)의 개수가 10만 개일 때 embedding table의 size는 300 X 10만으로 모든 값을 update하기에는 너무 무겁다. 그래서 **negative sampling** 기법을 사용한다. windows size 외에 있는 embedding vector 중 정해진 개수의 ramdom sample만 update한다. 그래서 엄청난 계산량을 줄여준다.

- window 내에 있는 sample: positive sample

- window 외에 있는 sample: negative sample

그래서 positive/negative label의 classification으로 문제가 바뀌었다고 볼 수 있다.

**성능**

Skip-Grams 이 CBoW보다 성능이 더 좋다.

전자는 1개의 단어로 4개의 단어를 update하는데, 후자는 4개의 단어들로부터 1개 단어와 연관된 weight를 update한다. update가 더 많이 이루어지는 전자가 더 성능이 좋다.

## Property of Word Embedding

<img src="https://user-images.githubusercontent.com/68107000/171753047-07328cf3-d12d-4df1-84a9-6e20efcb6725.png" title="" alt="image" data-align="center">

man과 woman의 유사도로 king과 같은 유사도를 보이는 queen을 유추할 수 있다.

단어의 유사도로 새로운 단어 유사도를 예측할 수 있기 때문에 Language model이 편해진다.

## Pretrained Word Embedding

word embedding table은 크기가 크고, 굉장히 큰 말뭉치로 학습된다. 

직접 학습시키기 어려운데, 이미 학습시켜 만들어진 pre-trained word embedding table을 다운받아 사용할 수 있다.

transfer learning을 하는데 사용된다.

예를 들어 내가 원하는 chatbot의 language model 네트워크를 구성하고 embedding 하는 과정이 필요한데, pretrained embedding table을 다운받은 후 추가적으로 원하는 학습을 적은 데이터셋으로 시켜 원하는 목적의 챗봇을 얻는다.

## Conditional Language Model: Translation

Language Model:

![image](https://user-images.githubusercontent.com/68107000/171847635-9e71a554-0a35-4033-ada6-70238c329f67.png)

Language Model은 앞 단어가 input이 되어 그 다음 단어를 예측한다.

-----

기계 번역(Seq2Seq):

![image](https://user-images.githubusercontent.com/68107000/171847661-e65d057d-c321-4d23-b966-b1da2e2069ad.png)

![image](https://user-images.githubusercontent.com/68107000/171847275-02ff6bdb-e049-469b-ab4c-07fa3c2de6d0.png)

기계 번역(Seq2Seq)은 "**프랑스어 단어들 (문장)이 주어졌을 때**" 특정 단어들이 나오는 확률 분포 찾는 문제이다. **조건부가 붙는다.** 번역은 다양하게 될 수 있는데 그 중 **가장 확률이 높은 단어들의 조합을 찾아야 한다.**

### Greedy Search

![image](https://user-images.githubusercontent.com/68107000/171847562-a9c9e647-e529-44c4-87d1-d6abb2c13c7b.png)

softmax를 거친 output이 나오고, 가장 높은 확률을 가진 단어를 output으로 선택한다.

각 stage로 모든 단어 한 번씩만 search해서 속도는 빠르지만 greedy 해서 최선이 아닐 수 있다. 다른 번역 결과가 발생할 수 있다.

### Beam Search

![image](https://user-images.githubusercontent.com/68107000/171847409-737264fe-7365-4c6c-8547-62671c40942e.png)

greedy와 비슷하게 heuristic한 방법이다.

k: beam size로 뒀을 때, 가장 높은 확률 k로 유지하면서 이어나간다.

## Evaluating Machine Translation

정량적으로 번역의 질을 어떻게 평가할 수 있을까?

candidate는 컴퓨터가 번역한 문장이고 reference는 전문가가 번역한 문장이다.

### Unigram Precision

다양한 전문가 문장에 등장한 ca의 단어 수 / ca의 모든 단어 수

문제점: 만약 candidate: the the the the the the the 일 경우, 모두 count 되어 1이 된다.

### Modified Unigram Precision

중복을 계산한다.

min(전문가 문장에 등장한 ca의 단어 수, **하나의 전문가 문장에서 중복되지 않고 동일하게 등장한 ca의 단어 수**) / ca의 모든 단어 수

```
candidate: the the the the the the the
reference: the cat is on the mat

→ 2/7
```

문제점: 단어의 순서는 전혀 고려되지 않는다.

### Modified n-gram Precision

n개의 단어로 묶었을 때 전문가와 동일하게 등장하는 gram 수(중복 고려 O) / n개의 단어로 묶었을 때 count되는 gram 수

참고로 n=2이면 bigram이다.

```
candidate: the the the the the the the
reference: the cat is on the mat

→ 0/6 = 0
```

```
candidate: the cat the cat on the mat
reference1: the cat is on the mat
reference2: there is a cat on the mat
→ 4/6
```

문제점: 기계 번역이 짧아지면 precision이 높아지는 경향이 있다.

### BLEU Score

![image](https://user-images.githubusercontent.com/68107000/171850784-9a42e094-1ac0-4902-8c0b-caedc15d43b6.png)

Bilingual Evaluation Understudy: 최종 기계 번역의 정량적 질을 판단할 수 있는 metric

BP: brevity penalty의 약자

만약 N이 4라면 unigram, bigram, trigram, 4-gram에 대해서 4개를 모두 계산하고 각 n-gram에 대한 가중치를 다르게 하여 곱해 더할 수 있다.

BP는 컴퓨터로 번역된 문장이 길다면 패널티를 전혀 주지 않지만,

짧거나 같은 길이의 문장으로 번역되었다면 1보다 작은 값의 정수를 곱해 페널티를 주게 된다.

## Summary 추가하기!

여기 계산문제는 꼭 나오니... 예제를 찾아볼 필요가 있다!
