# Transformer Models

## Attention is a General Technique

attention은 기계 번역에서 굉장한 잠재력을 지니고 있다. 그렇다고 해서 attention은 자연어 처리에서만 쓰는 것이 아니다. CNN을 포함한 많은 아키텍쳐에 쓰이고 있다.

attention에 대한 일반적인 정의는 다음과 같다.

일련의 vector value들과 vector query가 주어졌을 때 attention은 value에 weighted sum해서 query에 dependent한 attention을 찾아야 한다.

우리는 query가 value에 attend한다고 표현한다.  value는 번역하고자 하는 단어, query는 대응되는 값으로 생각할 수 있다.

![image](https://user-images.githubusercontent.com/68107000/172035353-44bf5f50-3de9-4107-a619-a8cb6d857eb9.png)

weighted sum이란 value에 포함한 정보를 선택적으로 요약하는 것이다. 이 때, query는 어떤 value에 집중할지를 결정한다.

person에 대응되는 각각의 value들에 대해서 weighted sum 한 것이 attention score이다.

## Transformers: Attention is All You Need

- 기존에 기계 번역은 rnn (lstm) + attention 으로 수행했는데 transformer는 rnn 부분을 다 제거하고 어텐션만 사용한다. "어텐션 밖에 필요 없다!"

- 트랜스포머의 등장으로 기계 번역에서 새로운 SOTA 성립

## Transformers: Model Architecture

![image](https://user-images.githubusercontent.com/68107000/172747291-0dcdc893-f001-41a2-b309-89ee7822cdca.png)

## Transformers: Basic Self Attention

input: sequence of tensors, (실제로는 vector), x1\~x4로 프랑스어 단어에 대응되는 벡터
output: sequence of tensors, each one a weighted sum of the input sequence, y1\~y4로, 결과적으로 attention score 값

y2 vector에 대응되는 attention 값을 구하고 있다.

Self Attention: decoder 없이 자기 자신을 상대로 attention 수행한다는 것을 말한다.

따라서 self attention에는 학습이란 개념이 전혀 들어가지 않는다.

## Transformers: Query, Key, Value

학습 가능하게끔 만들고자 한다.

input vector x_i가 3가지 방식으로 쓰인다.

- query: query를 만드는데 쓰인다. 어떤 vector에 대한 attention을 계산할거냐

- key: attention weight를 만들기 위해서 쓰인다. attention weight를 계산하기 위해서 비교되고자 하는 값들

- value: weighted sum 값을 만들기 위해 필요한 input vector

vector 3개로 조합해서 attention을 구한다.

![image](https://user-images.githubusercontent.com/68107000/172750829-bd29b097-ff1b-4bf1-93e8-88827743d248.png)

input과 weight의 matrix multiplication으로 query, key, value를 만든다.

우리는 사진에서 색칠되어있는 3가지 weight matricies을 훈련시키면 attention을 훈련시키는 것과 동일하게 된다.

query과 key를 곱하면 (내적하면) weight 값이 나오게 되고 (그림에서 w_23)

이 weight 값과 value (V2)를 곱해 weighted sum 한 게 최종 output(y2)이 된다.

## Transformers: Multi-Head Attention

여러 개의 Self Attention이 병렬적으로 존재한다.

![image](https://user-images.githubusercontent.com/68107000/172747376-e525ed34-4415-46ee-81e6-1f60c78a3535.png)

그림처럼 attention이 3개일 때 (h = 3)

- 쿼리 매트릭스가 3개 

- 웨이트 매트릭스 각각 다른 게 3개

- value 매트리스 각각 다른 게 3개 있다.

   ![image](https://user-images.githubusercontent.com/68107000/172746828-a40ec45d-4202-4992-a2a3-c59a046eea3d.png)

- **다양한 관점에서 어텐션을 수행하기 위해 사용한다.**

- 예를 들면, 문장 안에 있는 it이 가르키는 것이 모호한데, attention을 한 번만 하면 가리킨 대상이 부정확할 수 있다. attention 3개를 병렬로 해 다양한 관점에서 가장 많이 가리킨 것으로 결정할 수 있다.

## Transformers: Layer Normalization

<img src="https://user-images.githubusercontent.com/68107000/172747008-716ff78e-7411-4031-9220-aab2f2cbeb89.png" title="" alt="image" width="414">

두 사진 모두 gradient descent에 의해 최저점으로 가고 있지만 mean과 stardard deviation이 uniform하지 않을 때보다 **uniform 할 때 빠르게 수렴함**

![image](https://user-images.githubusercontent.com/68107000/172747043-b29f55a1-94b5-47b3-859e-b7036aedb84a.png)

마찬가지로 transformers 에서도 input이 네트워크들(attention과 FC layer)을 지날 때마다 mean과 stardard deviation이 엉망이 된다.

**layer normalization으로 mean과 stardard deviation 똑바로 재정렬을 시켜준다.**

layer normalization은 batch normalization과 다르게 C, H, W 방향으로 normolization

- batch normalization: 이미지와 채널에 대해서 다른 서로 다른 input 샘플을 가지고 normalization 

- layer normalization: 같은 샘플 내에서 하나의 layer에 존재하는 W, H, C 데이터들을 다 normalization

## Transformers: Positional Embedding

![image](https://user-images.githubusercontent.com/68107000/172746964-5751d080-7d3a-44a0-bcb7-17d045cb15d1.png)

- 지금까지 시퀀스의 순서를 고려하지 않았으므로 순서가 결과에 영향을 미치지 않았다.

- 실제 우리 언어는 단어의 순서가 굉장히 중요함. 단어의 순서에 따라 의미가 많이 변한다

- 그렇기 때문에 각 input 벡터에 대해서 포지션을 구해주어야 한다.

- person hat → person 첫 번째 위치에 있고 hat이 두 번째 위치에 있다라는 걸 알려주기 위해서 input sequence를 word embedding 한 다음 position embedding을 더해준다. 

- transformer block을 거쳐 결과를 얻는데 사인 코사인 값을 이용한 상대적인 포지션 베딩을 수행한다.

## Transformers: Last Trick

![image](https://user-images.githubusercontent.com/68107000/172746920-9a062a50-a1a7-43eb-ad57-b0d107931d5e.png)

transformer는 input을 한 번에 다 보는데, decoder에서 다음 벡터를 차례로 예측해야하는 경우 미래의 값을 attention하면 안된다.

그래서 attention weight에 mask를 씌워, 현재 attention y1은 현재(x1)와 과거(x0) 값만 볼 수 있도록 해준다.

## Transformers: Remarks

transformers 모델이 성공할 수 있었던 이유

- self-attention: 매우 긴 문장에 대해서도 encoding을 한 번에 다 하기 때문에
  
  - 순차적으로 했을 때 일어날 수 있는 infomation loss를 없앨 수 있음.

- 출력 시 unlabeled 데이터셋으로 self-supervision으로 수행. 그래서 language model을 만드는데 적합한 구조가 됨.

학습

1. Pre-training on large unlabeled datasets (self-supervised learning)
   
   - I have an apple 이라는 데이터가 있을 때, I만 넣었을 때 have가 오도록, I have가 있을 때 an이 오도록, I have an이 있을 때 apple이 오도록 training 할 수 있음.
   
   - I have an 이라는 label이 존재하지 않아도 자기 데이터만으로 training을 수행할 수 있음.

2. Training for downstream-tasks on labeled data (supervised learning) → Fine-tuning
   
   - 실제 specific한 NLP task에 대해서 supervised learning을 하고 이 과정을 Fine-tuning이라고 한다.
   
   - 예, 기계 번역: pre-trained 된 것에 덧붙여 I have an apple 은 나는 사과를 가지고 있다 라는 뜻의 label을 가진 데이터셋으로 학습시킴

## BERT

Bidirectional Encoder Representations from Transformers

- Multi-layer **bidirectional** transformer encoder
  
  - decoder 없이 **encoder만 사용함.**
  
  - unidirection의 경우 앞 단어가 뒷 문맥에 활용되지 않았음.
  
  - bidirection을 통해 뒷 정보가 앞으로도 흐르게 됨.

- Architecture almost identical to original transformer except: (training 시 transformer와 훈련방법이 다른데,)
  
  - bidirectional masking (Cloze task)
  
  - next sentence prediction as additional pre-training task

- pre-training dataset: BookCorpus (8억개 단어), Wikipedia(25억개 단어)

- 위의 데이터셋으로 self supervised pretraining을 수행한다

### BERT input

![image](https://user-images.githubusercontent.com/68107000/171967156-d56e7fbe-c9e3-4f95-aee2-5fc82a703328.png)

BERT의 input은 3개의 embedding으로 이루어진다. input은 뒤 문장들로 classification을 수행해줄 것이라는 cls 토큰이 있고, 문장을 구분해주는 sep 토큰이 있다.

## BERT Pre-training Tasks

### 1. Masked language model

일반적인 transformer 는 NWP으로 순차적 학습을 진행한다. 예를 들어 a → quick, a quick → brown 이 등장할 것이라고 학습시킨다.

input sequence에서 15%의 단어를 mask 한다. 

15% 단어 중에서!

- 80%는 `[MASK]` 라는 스페셜 토큰으로 바꿔 빈칸 채우기처럼 close task로 BERT가 맞출 수 있도록 training 시킨다.

- 10%는 `랜덤 단어`로 바꾼다. 이는 regularization이라고 볼 수 있다.

- 10%는 fine-tuning 시나리오를 흉내내기 위해 `원래 단어` 그대로 둔다.

이렇게 학습시키면, BERT는 `[MASK]`를 높은 확률로 원래 단어로 예측할 것이다.

### 2. Next sentence prediction

Balanced(50% IsNext, 50% NotNext) binary classification task를 수행한다.

supervised learning이다.

```
Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
Label = IsNext
```

```
Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
```

the man \~ store 문장 다음에 he bought \~ 문장이 올 것이다 → isNext

the man ~ store 문장 다음에 penguin ~ 문장이 올 것이다 → NotNext

## BERT Architecture

![image](https://user-images.githubusercontent.com/68107000/171967343-a1b0afb2-5087-450e-95b2-ff0f0ae0f1c7.png)

3억 4천만 개의 파라미터가 존재한다.

transformer block 24개, attention head 16개로 구성되어 있다.

## BERT Fine-Tuning

BERT를 Fine-Tuning할 때는 기존에 pre-training된 것 language model은 얼려둔다. 즉 weight가 더이상 훈련이 안되게 한다.

그리고 one or more layers를 추가해 downstream task를 위한 labeled dataset으로supervised learning을 수행시킨다. 

추가된 layer만 훈련시키기 때문에 fine-tuning은 굉장히 빠르게 수행된다.

원하는 목적의 챗봇을 만들 수도 있고, 감정 분류 모델을 만들 수도 있다.

## GPT

Generative Pre-Trained Transformer

- OpenAI가 개발

- Unidirectional: 일반적인 transformer encoder, decoder 형태와 같이 next word를 예측하도록 훈련시킨다.

- 종류, 파라미터수가 기하급수적으로 늘어난다.
  
  - GPT: 110 Million
  
  - GPT-2: 1.5 Billion
  
  - GPT-3: 175 Billion (초거대 AI)

- key concept: labeled data가 부족한 문제를 다음 2개의 training process로 해결한다.
  
  1. Generative pre-training on unlabeled data (self-supervised learning)
     
     - NWP로 pretraining
  
  2. Discriminative fine-tuning on labeled data (supervised learning)
     
     - downstream task에 대해서는 labeled data로 파인튜닝

- 거대한 BookCorpus 데이터셋으로 pre-training (7000 books)

- Based on decoder architecture from original Transformer
  
  - decoder가 순차적인 부분이니까 decoder만

## GPT Architecture and Downstream Tasks

![image](https://user-images.githubusercontent.com/68107000/171969720-1b4808c2-d400-407d-90eb-de3b0343e4b5.png)

## Summary

- Transformers
  
  - "Attention Is All You Need" 라는 논문에서 기존의 RNN + Attention 구조에서 순차적인 RNN 부분을 뺀 모델을 제시한다. 
    
    Ashish Vaswani et al. (Google Brain), NeurIPS 2017
  
  - 병렬적으로 문장의 모든 word를 동시에 encoder 부분에서 처리할 수 있다.
  
  - 따라서 속도가 빨라지고, 모든 맥락을 보기에 기계 번역 성능도 좋아졌다.

- BERT
  
  - "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"는 encoder 부분만 사용하는 모델이다.
  
  - Jacob Devlin et al. (Google AI Language), arXiv 2018
  
  - 학습: Masked LM (self supervised로), NSP (supervised classification task로)

- GPT
  
  - "Improving Language Understanding by Generative Pre-Training"는 순차적 구조인 decoder만 사용하는 모델이다. unidirectional 하다.
  
  - Alec Radford et al. (OpenAI)
  
  - training: NWP
  
  - 구조가 최근 초거대 AI 만드는데 더 유리한 구조이다.

- 비교
  
  - 비슷한 시기에 나왔지만 같은 파라미터 개수 대비 GPT보다 BERT가 성능이 훨씬 좋다.
  
  - downstream에 대해 fine-tuning하는 것은 bert와 동일하다.
