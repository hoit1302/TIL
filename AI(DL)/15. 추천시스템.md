# 추천 시스템

information filtering technique

각 사람들이 흥미있어할만한 정보를 제공해줌.

e-커머스, 광고, 제안 등 실생활에서 많이 활용되고 있다.

- google play에서 40%는 추천에 의해 앱을 다운 받는다

- youtube에서 60%는 추천에 의해 시청된다.

deep neural network가 최근 추천 시스템에 활발히 활용된다.

## Recommendation System Architecture

![image](https://user-images.githubusercontent.com/68107000/172077502-1d9c7bc9-5b20-4a5a-9a26-cb5928a8c35c.png)

- candidate generation: 거대한 corpus로부터 작은 부분 집합으로 후보군을 만드는 시스템
  
  - DNN 활용됨.
  - 이 부분에 대해서 살펴보는 내용이 본 내용

- scoring: 후보군들로부터 실제 유저가 볼 수 있는 후보들로 조금더 정확히 select하는 시스템
  
  - hand-engineering으로 각 회사마다 다양하게 구현함.

- re-ranking(순서 조정): 마지막으로 dislikes, diversity, freshness 등 추가적인 요소를 고려하는 시스템
  
  - 이 부분 역시 hand-engineering

## Similarity

item과 query의 유사도를 어떻게 수학적으로 측정할까?

s(q, x): query q와 item x의 유사도, q와 x는 같은 embedding space에 존재함

- Cosine: ![image](https://user-images.githubusercontent.com/68107000/172091969-a74a190e-5e3e-4bc5-aa39-80581ff1aec0.png)

- Dot Product: ![image](https://user-images.githubusercontent.com/68107000/172092011-5813e695-aab4-4d77-9c45-7452992c9fd1.png)

- Euclidean distance: ![image](https://user-images.githubusercontent.com/68107000/172092026-8a3442bf-1a23-4ae6-8ddb-cb979f6f5be1.png)

## Content-Based Filtering

items 간의 similarity를 사용해 유저가 좋아할만한 아이템을 추천한다.

어떤 유저가 아이템 1번을 좋아하면 1번과 유사한 2번을 추천한다.

item을 나타내기 위해 hand-engineered features을 사용한다.

| item\feature | a   | b   | c   | d   | e   |
| ------------ | --- | --- | --- | --- | --- |
| 1            | 1   | 0   | 0   | 1   | 0   |
| 2            | 0   | 1   | 1   | 0   | 0   |
| 3            | 0   | 0   | 1   | 0   | 1   |
| 4            | 1   | 0   | 0   | 1   | 1   |

만약 user A가 itme 4를 좋아한다면, dot product로 similarity를 계산했을 때 어떤 아이템을 추천받을까?

동시에 1인 특징의 개수를 세면 된다.

- s(4, 1) = 2

- s(4, 2) = 0

- s(4, 3) = 1

따라서 item 1을 추천받을 것이다.

**한계점**

- 사람이 직접 설계(hand-engineered feature)하기 때문에 feature 설계가 성능에 의존된다.

- domain-specific knowledge가 있는 사람만이 특징을 선정할 수 있다.

- 과거에 존재하는 흥미 요소를 기반으로 추천해준다. 다른 요소가 고려가 되지 않는다.

## Collaborative Filtering

쿼리와 item의 유사도를 동시에 사용해 추천해준다.

여기서 쿼리란 추천 해주기 위해 시스템이 사용하는 정보이다. 예를 들어 성별, 나이, 기록, 피드백 등

A 유저가 아이템 1번을 좋아하고 B 유저가 A 유저와 취향이 비슷하다. 만약 B가 아이템 2번을 좋아하면 A에게 추천하게 된다.

더 많은 정보를 보고, 성능이 더 좋다.

embeddings는 hand-engineered feature가 아니고 기계에 의해 학습된다.

Collaborative Filtering 방법으로 embedding matrix를 얻기 위해서 2가지 방법이 있다.

- using matrix factorization

- using DNN, 해당 방법을 최근 굉장히 활발히 사용하고 있음.

## Matrix Factorization

**피드백 matrix A**

**피드백 matrix A** ∈ ℝ (m X n) - *user m명, item n개*는 다음의 정보로 얻을 수 있다.

- 간접적으로, 유저의 과거 기록을 보면서

- 직접적으로, 유저의 좋아요/싫어요 피드백을 수집해서

**UV transpose matrix**

2\. item embedding matrix V ∈ ℝ (n X d). item n개, dimention d

3\. user embedding matrix U ∈ ℝ (m X d). user m개, dimention d

4\. product of 2 embedding matrices U (m X d) V transpose (d X n) ∈ ℝ (m X n)

embeddings을 얻기 위해 학습해 얻을 수 있는 `UV transpose matrix`는 피드백 matrix A의 좋은 근사치이다.

![image](https://user-images.githubusercontent.com/68107000/172093915-c2fe7eae-b2b2-464a-9c6b-351c7da581b7.png)

`UV transpose matrix`는 수식을 minimize하여 얻을 수 있다.

주의할 게 0이 아닌 value에 대해서만 sum한 값이 최소화되도록하는 일종의 optimization problem이다.

neural network에서 SGD로 minimize 했듯이 **weighted matrix factorization** 을 활용해 문제를 푼다.

**한계점**

training set에 존재하지 않는 MBTI와 같은 새로운 feature에 관한 embedding table은 만들 수 없다. 혹은 어렵다. 다시 해당 정보를 포함한 matrix를 만들어 factorization해줘야 한다.

user specific한 정보들은 training에 포함할 수 없다. 따라서 일반적인 유명한 item이 모든 사람에게 추천되고 개인화된 추천이 어렵다.

하지만 **dnn은 inpuy layer가 유연**하다!. 따라서 추천시스템에서 각광받고 있다.

## DNN for Recommendation

- input: queries
  
  - 어떠한 query도 다 됨.
  
  - watch time (dense features):시간이 dense 한 숫자임
  
  - watch history (sparse features): 봤던 영화가 훨씬 적어서

- output: 각 item에 관심있을 확률
  
  - 해리 포터를 볼 확률
  
  - class는 아이템 개수만큼 존재하기 때문에, multiclass prediction 문제

![image](https://user-images.githubusercontent.com/68107000/172077319-e4550fdf-f9c7-49d7-9c81-4ded5815bba4.png)

![image](https://user-images.githubusercontent.com/68107000/172095219-ed6e03ad-52d0-4ce5-bf2b-1f917794fb52.png)

x: input layer

DNN의 가장 마지막 hidden의 output **y**

- query x에 대한 embedding vector

- embedding의 dimention인 d

embedding matrix V는 (n X d) 크기이고 softmax layer 마지막에서 보이는 dimention은 item 개수 n

V_j: item j에 대한 embedding vector

embedding이 2번 일어난다.

쿼리에 대한 embedding vector를 얻는 embedding 과정은 DNN를 사용하고 최종 probability p를 얻기 위해서 사용되는 embedding matrix V는 softmax layer에 있는 건데, 해당 matrix의 하나의 row가  (V_j) embedding vector이다.

matrix factorization에서는 

embedding matrix dimention을 직접 다 정해서 만들어줬었고 그걸 수학적으로 곱해서 정해진 feedback 매트릭스와 유사하게 만들어줬었죠

DNN는 

이와 다르게 input에 flexiblility가 있고 형성된 부산물들이 embedding vector가 된다.
