# Recurrent Neural Network

## Neural Networks that process sequences

앞서 배웠던 CNN은 image와 같은 spatial data를 잘 처리함

시간 상 순서가 있는 데이터에 좋은 성능을 냄

- temporal data

- sequential data

→ speech, language, video

### ![image](https://user-images.githubusercontent.com/68107000/171523814-ed84386e-b29c-44be-8f6a-2d1e9a951764.png)

주황색 input, 녹색: hidden, 파랑색: output

첫번째 네트워크를 제외한 나머지 네트워크는 가로축으로도 늘려져 있는데, 가로축은 t 시간을 의미한다. hidden state의 정보가 시간 상에서 전달된다.

input이 sequential data로 이뤄질 때도 있다.

image captioning: 이미지를 설명하는 문장을 생성, 단어 하나하나가 output

action prediction: 자율주행에서 과거의 방향들(in)로부터 다음 방향 핸들(out) 결정

video captioning: 영상을 설명하는 문장을 생성

OpenAI GPT-3: 가장 큰 NN, 굉장히 향상된 챗봇

### Idea

> **internal state (hidden state, memory)를 가지고 있어** 다양한 길이를 가지는 sequence of inputs을 처리할 수 있다.

## Simple(Vanilla) RNN

### Math

**hidden state를 매 time step마다 update한다.**

![image](https://user-images.githubusercontent.com/68107000/171525051-4815b1d9-1b5a-46da-aebe-cab9ea072b3a.png)

시그마는 activation function을 의미한다. weight matrix는 3개이고 bias는 2개이다. weight matrix와 bias는 모두 trainable 하다.

이전 input에 대해서 가중치를 두고 현재 input에 가중치를 두어 계산하여 hidden state를 얻는다.

### Unrolled

![image](https://user-images.githubusercontent.com/68107000/171526180-f628e8a7-85bd-4ed5-9d58-8af76193ac00.png)

unrolled: time step에 대해 풀어서 표현

many-to-many의 형태이다.

앞선 수식에서 보았듯이, 3개의 weight (와 2개의 bias)에 대해서 1번 training하여 고정이 되어 모든 time step에서 같은 weight matrix(와 bias)를 사용한다.

그리고 L1~Lt 를 더해 하나의 Loss 값을 얻게 된다.

### 예시, **Character-Level Language Model**

![image](https://user-images.githubusercontent.com/68107000/171526434-56f6200b-7b82-46bf-bc83-ca9dfdb06acf.png)

input으로 h만을 넣어주면 나머지 input은 자동으로 output이 input으로 들어간다. 왜냐하면 hello가 되도록 training 했기 때문이다.

## Multi-Layer RNN

![image](https://user-images.githubusercontent.com/68107000/171524356-5b693dc4-471d-4719-bd3e-6815c3a6dd1d.png)

가로축은 unrolled되어 시간 t를 의미하고 세로축은 depth를 의미한다. 세로축으로 hidden layer가 하나였는데, 여러 개로 확장되었다. 많이 사용된다고 한다.

## Gradient Vanishing problem in RNN

바닐라 RNN은 GVP이 매우 심각해서 현재 전혀 사용되고 있지 않다.

![image](https://user-images.githubusercontent.com/68107000/171524768-eb7cbea2-aa8a-48a3-bb33-1df2c419030a.png)

d h_t/d h_(t-1) 값은 결국 소수 X 소수 값이다

아래 사진처럼 문장이 길어지면 작은 소숫값을 더 많이 곱하게 되므로 더 잘 이뤄지지 않는다.

## LSTM (long short term memory)

RNN에서 STM가 hidden state였다.

cell이 LTM을 수행하는데, LTM 혹은 현재 들어오는 input x_i가 STM에 얼마나 반영되지를 gate로 조절한다.

![image](https://user-images.githubusercontent.com/68107000/171543176-a347732c-f32a-4d31-9786-2710f3b87e17.png)

이전 hidden state와 현재 input에 가중치를 곱하고 더해 계산이 이루어진다. 이를 weighted sum이라고 말한다.

⨀는 element-wise 곱이고 weight는 8개, bias는 4개로 이루어져있다.

- Forget gate: cell을 지울지말지, 0이면 c_t-1인 과거 메모리를 완전히 지워서 반영이 안되게 된다.

- Input gate: cell에 input을 쓸지말지

- Output gate: 현재 c_t memory cell이 hidden state에 반영이 얼마나 될지

cell은 값을 어떤 특정 임의의 시간동안 기억한다. 메모리의 역할을 한다.

### LSTM Gradient Flow

![image](https://user-images.githubusercontent.com/68107000/171549447-73f51a6c-7e4c-46d9-9ee5-ff8fba2f8587.png)

C_t가 거치는 길을 보면, + 기호와 X 기호가 있다. +는 영향을 받지 않고 그대로 전달될 수 있지만 작은 소수의 값을 곱하다보면 Gradient Vanishing Problem이 있지 않을까? 하는 의문을 가지게 된다.

f_i를 training을 통해 거의 1의 값을 가지도록 훈련시킬 수 있다. (0.997)

즉 forget gate가 높을 값을 가지도록 학습시켜서 이전 cell의 값을 그대로 반영한다.

따라서 LSTM은 long term memory를 거의 항상 가지고 있고, 긴 문장에 대해서도 앞 단어를 잊어버리지 않고 처리할 수 있다.ㅌ`

## Seq2Seq - Language Translation Example

problem of **RNN-based** Seq2Seq

참고로 RNN-based란 vanilla RNN 혹은 LSTM 계열들을 이야기한다.

정보가 context vector로 압축되는 과정에서 bottleneck 된다. 즉 정보가 손실된다.

그래서 긴문장을 잘 번역해보기 위해서 Attention이 제안되었다.

## Attention

**Attend the important and relevant words** from the input sentence and 
**assign higher weights** to these words

뒤 단어가 앞 단어와 관련 있다는 맥락을 반영하기 때문에 간단한 예시로 단어의 순서를 바꿔 번역하는 것을 가능케한다.

기존의 NW에서 사용할 수 있는 도구로 생각하면 된다.

### Attention in Seq2Seq

전체 흐름: input → attention score → attention distribution → attention value → hat (output) 

![image](https://user-images.githubusercontent.com/68107000/171546110-5eb1053a-14a8-4d4e-ba86-d8a04334fd66.png)

hat으로 번역의 결과가 나와야하는 상황이다.

1. hidden state 3 vector와 encoder에 있는 hidden state z vector들과의 attention score를 계산한다. → e

2. wearing 이란 단어와 프랑스어 중에서 어떤 게 가장 연관도가 높은지 score를 계산하게 되고 softmax를 거쳐 확률로 바뀜. 원하는 output hat과 대응되는 프랑스어는 chapeau인데, 가장 값이 높은 걸 볼 수 있다.

3. 매 time step마다 attention 기반으로 다른 context value가 만들어져서 decoder로 들어간다. (이전에는 전체 time step에 대해 하나의 context value를 구했음)

따라서 정보 손실은 적어지고 번역 성능이 좋아진다. 

#### 1. Getting Attention Score

![image](https://user-images.githubusercontent.com/68107000/171546214-36182891-02ac-4817-9e5a-c9d22c1e6714.png)

h3에 대한 output을 원하기 때문에 h3와 각각의 encoder의 hidden state (z_1~4)를 곱해 하나의 scalar 값을 얻는다.

#### 2. Getting Attention Distribution

![image](https://user-images.githubusercontent.com/68107000/171546467-35be93b7-d183-484a-9070-9a2bb39958d8.png)

softmax 함수를 거쳐 구하게 된다.

#### 3. Getting Attention Value

h3 (output 얻고 싶은 부분에 해당하는 decoder hidden state) 에 대응되는 attention value C3 구하기

![image](https://user-images.githubusercontent.com/68107000/171546553-542024dc-e005-4299-a3d4-f008c96c537c.png)

각각 곱한 것을 모두 더해서 얻을 수 있다.

#### 4. Calculate Output

![image](https://user-images.githubusercontent.com/68107000/171546593-9820dd7a-a5be-4094-b778-211e0363ea61.png)

concat한 다음 W_c (학습을 통해 얻어질 수 있는 weight matrix) 와 곱해 얻어진 결과는 hat 단어와 대응된다.

## Summary

RNN은 GVP이 심각해서 LSTM이 등장했다. cell 들이 LT 메모리를 가져서 input, forget, output gate로 조절해준다. RNN과 LSTM으로 Seq2Seq를 꾸미면 일반적으로 enc → ◻ → dec 이런 구조인데 중간에 축약되어 번역 질이 떨어진다. 그래서 Attention이 등장해 관련 높은 단어들의 가중치 두게 되었다.
