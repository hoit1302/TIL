# Training CNN

## CONV Forward (in Python)

![image](https://user-images.githubusercontent.com/68107000/164568418-95518021-25c8-47c9-be5c-c85242b50654.png)

ìœ„ 3ê°œ `W2`, `H2`, `K`ëŠ” output ì „ì²´ë¥¼ ë‹¤ ê³„ì‚°í•˜ê¸° ìœ„í•¨ì´ê³ 

ë°‘ 3ê°œ `C`, `F`, `F`ëŠ” outputì˜ í•œ ì¹¸ì„ ìœ„í•´ ê³„ì‚°í•˜ê¸° ìœ„í•¨ì´ë‹¤.

## CONV Backpropagation

![image](https://user-images.githubusercontent.com/68107000/164950155-fd178b74-1a90-4708-b970-8e980cf62c01.png) 

## Maxpooling Backpropagation

![image](https://user-images.githubusercontent.com/68107000/164570192-5d236443-7993-42f9-a03c-7fa9b9280656.png)

maxì— í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ dO ì „ë‹¬ëœë‹¤.

## Batch Gradient Descent in CNN

DNN: Across an entire of training dataset, ê° training sampleì— ëŒ€í•´ì„œ

CNN: For **the entire** training samples

![image](https://user-images.githubusercontent.com/68107000/164570855-bcf5fd90-f9c4-40a7-ab06-0a40e5946409.png)

âˆ‘: training sample ì „ì²´ì— ëŒ€í•œ lossë¥¼ ë”í•˜ëŠ” ë¶€ë¶„ì´ë‹¤.

Î»R: regularization factor ë¥¼ ì˜ë¯¸í•œë‹¤.

í•˜ì§€ë§Œ sampleì´ ëª‡ ë§Œ ê°œë¼ë©´ í•œë²ˆ weight updateë¥¼ ìœ„í•´ ëª‡ ë§Œ ë²ˆì˜ forward ì—°ì‚°ì„ í•´ì•¼í•œë‹¤. ë§¤ìš° ì˜¤ëœì‹œê°„ì´ ê±¸ë ¤ ë¹„í˜„ì‹¤ì ì´ë‹¤.

ì „ì²´ training sampleì— ëŒ€í•´ì„œ 1ë²ˆ update

## Stochastic Gradient Descent in CNN

ë”°ë¼ì„œ ì¼ë¶€(mini-batch sizeë§Œí¼) training sampleë¡œ weight updateë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ `SGD`ë¥¼ ì‚¬ìš©í•œë‹¤. 

`SGD`ëŠ” mini-batchê°€ 1ì¸ ê²½ìš°ë¥¼ ì˜ë¯¸í•˜ê¸°ë„ í•˜ëŠ”ë° ì—¬ê¸°ì„  `SGD` ì¤‘ì—ì„œë„ `mini-batch G.D.`ë¥¼ ì˜ë¯¸í•œë‹¤. mini-batchë¥¼ stochastic(í™•ë¥ ì ì¸)í•˜ê²Œ ì–»ëŠ”ë‹¤.

mini-batch sizeë§Œí¼ì„ ëœë¤í•˜ê²Œ ë½‘ëŠ” 

```textile
1. Initialize the weights/biases to small random numbers.

2. For a mini-batch of randomly! chosen training samples:
  1. Forward propagation: calculate the mini-batch of output values.
  2. Compute loss ğ¿.
  3. Backpropagate errors through network.
  4. Update weights/biases
3. Repeat Step 2 until it covers the entire training samples (1 epoch).

4. Repeat Step 2, 3 until network is trained well.
```

![image](https://user-images.githubusercontent.com/68107000/164570889-74eb8054-5d4f-433a-bdb2-0e6747bd73ac.png)

Mì€ mini-batch sizeë¥¼ ì˜ë¯¸í•œë‹¤.

ì´ ë°ì´í„°ì…‹ ê°œìˆ˜ê°€ 50000ê°œì´ê³  mini-batch sizeê°€ 100ì´ë¼ë©´ 500ê°œì˜ mini-batchê°€ ì¡´ì¬í•˜ê³ , weight updateë¥¼ 500ë²ˆ í•´ì•¼ 1 epochsë¼ê³  í•  ìˆ˜ ìˆë‹¤.

mini-batch 2ëŠ” mini-batch 1ì—ì„œ í•™ìŠµì‹œì¼°ë˜ ì…‹ì„ ëº€ ë‚˜ë¨¸ì§€ì—ì„œ ëœë¤í•˜ê²Œ ê³ ë¥¸ë‹¤.

## GD vs. SGD

![image](https://user-images.githubusercontent.com/68107000/164569632-90518257-4a05-470f-b7c5-77582171063c.png)

- `SGD`ì˜ training ì†ë„ê°€ í›¨ì”¬ ë¹ ë¥´ë‹¤.
  
  - `GD`ëŠ” 1ë²ˆ ì—…ë°ì´íŠ¸í•˜ëŠ”ë° ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¬ì§€ë§Œ 
  
  - `SGD`ëŠ” mini-batch sizeë§ˆë‹¤ ì—…ë°ì´íŠ¸í•œë‹¤.

- ì„±ëŠ¥ ë˜í•œ ì‹¤ì œë¡œ ë” ì¢‹ê²Œ ë‚˜ì˜¤ëŠ” ê²½í–¥ì„±ì´ ìˆë‹¤.
  
  - `GD`ëŠ” local minimumì— ë¹ ì§€ë©´ `G.D. = 0`, ë¹ ì ¸ë‚˜ì˜¬ ìˆ˜ ì—†ì§€ë§Œ (directí•˜ê²Œ)
  
  - `SGD`ëŠ” ëœë¤ì„±ì„ ê°€ì§€ê³  ìˆì–´ì„œ ë¹ ì ¸ë‚˜ì˜¬ ì—¬ì§€ê°€ ìˆë‹¤.

ì „ì²´ training datasetì— ëŒ€í•´ forward propagation í•œ í›„ lossë¥¼ êµ¬í•˜ëŠ” ë°˜ë©´ì—

mini batch ë‹¨ìœ„ë¡œ weight update ì§„í–‰
