# Training CNN

## CONV Forward (in Python)

![image](https://user-images.githubusercontent.com/68107000/164568418-95518021-25c8-47c9-be5c-c85242b50654.png)

위 3개 `W2`, `H2`, `K`는 output 전체를 다 계산하기 위함이고

밑 3개 `C`, `F`, `F`는 output의 한 칸을 위해 계산하기 위함이다.

## CONV Backpropagation

![image](https://user-images.githubusercontent.com/68107000/164950155-fd178b74-1a90-4708-b970-8e980cf62c01.png) 

## Maxpooling Backpropagation

![image](https://user-images.githubusercontent.com/68107000/164570192-5d236443-7993-42f9-a03c-7fa9b9280656.png)

max에 해당하는 것만 dO 전달된다.

## Batch Gradient Descent in CNN

DNN: Across an entire of training dataset, 각 training sample에 대해서

CNN: For **the entire** training samples

![image](https://user-images.githubusercontent.com/68107000/164570855-bcf5fd90-f9c4-40a7-ab06-0a40e5946409.png)

∑: training sample 전체에 대한 loss를 더하는 부분이다.

λR: regularization factor 를 의미한다.

하지만 sample이 몇 만 개라면 한번 weight update를 위해 몇 만 번의 forward 연산을 해야한다. 매우 오랜시간이 걸려 비현실적이다.

전체 training sample에 대해서 1번 update

## Stochastic Gradient Descent in CNN

따라서 일부(mini-batch size만큼) training sample로 weight update를 수행하는 방법 중 하나인 `SGD`를 사용한다. 

`SGD`는 mini-batch가 1인 경우를 의미하기도 하는데 여기선 `SGD` 중에서도 `mini-batch G.D.`를 의미한다. mini-batch를 stochastic(확률적인)하게 얻는다.

mini-batch size만큼을 랜덤하게 뽑는 

```textile
1. Initialize the weights/biases to small random numbers.

2. For a mini-batch of randomly! chosen training samples:
  1. Forward propagation: calculate the mini-batch of output values.
  2. Compute loss 𝐿.
  3. Backpropagate errors through network.
  4. Update weights/biases
3. Repeat Step 2 until it covers the entire training samples (1 epoch).

4. Repeat Step 2, 3 until network is trained well.
```

![image](https://user-images.githubusercontent.com/68107000/164570889-74eb8054-5d4f-433a-bdb2-0e6747bd73ac.png)

M은 mini-batch size를 의미한다.

총 데이터셋 개수가 50000개이고 mini-batch size가 100이라면 500개의 mini-batch가 존재하고, weight update를 500번 해야 1 epochs라고 할 수 있다.

mini-batch 2는 mini-batch 1에서 학습시켰던 셋을 뺀 나머지에서 랜덤하게 고른다.

## GD vs. SGD

![image](https://user-images.githubusercontent.com/68107000/164569632-90518257-4a05-470f-b7c5-77582171063c.png)

- `SGD`의 training 속도가 훨씬 빠르다.
  
  - `GD`는 1번 업데이트하는데 오랜 시간이 걸리지만 
  
  - `SGD`는 mini-batch size마다 업데이트한다.

- 성능 또한 실제로 더 좋게 나오는 경향성이 있다.
  
  - `GD`는 local minimum에 빠지면 `G.D. = 0`, 빠져나올 수 없지만 (direct하게)
  
  - `SGD`는 랜덤성을 가지고 있어서 빠져나올 여지가 있다.

전체 training dataset에 대해 forward propagation 한 후 loss를 구하는 반면에

mini batch 단위로 weight update 진행
