# Neural Network Basics

## Modeling Pavlov Using a Neuron

ppt 그림 보기, 첫번째 그림

![image](https://user-images.githubusercontent.com/68107000/164114686-ec20baad-8ca8-4076-b6ec-c7ac4a1ec10f.png)

## Mathematical Formulation of a Neuron

뉴런 하나만 가지고 있는 모델을 **Perceptron** 이라고 한다.

![image](https://user-images.githubusercontent.com/68107000/163997735-530bafa4-6989-4c43-baf1-3be527558e6a.png)

## Gradient Descent

생물은 STDP 로 학습됐다. 기계의 weights 들을 어떻게 학습시킬 수 있을까? → Gradient Descent로 학습시킬 수 있다.

A famous optimization algorithm to **minimize a cost function** by iteratively moving in the direction of **steepest descent** as defined by the **negative gradient**

steepest descent의 방향은 negative gradient의 방향이다.

<img src="https://user-images.githubusercontent.com/68107000/163999309-7dc65527-b8ad-4d6f-8730-cc6ed3b34744.png" title="" alt="image" data-align="center">예시: cost function C를 최소화하는 x를 찾아보았음.

$$
argmin_x(C = x^2)
$$

initial point는 random이다. 

## Training a Perceptron

Training a perceptron is solving an optimization problem with respect to weights.

error를 최소화하는 weights 들을 찾아야 한다.

학습 목표는 input from bell의 입력이 1일 때 output이 1이 되도록 하는 것이다.

1. Initialize the weights to small random numbers
   - 0으로 initial 하면 안됨.
2. For each training sample 𝒙(𝒌):
   - 각각의 새로운 training sample에 대해서 진행한다.
   1. Calculate the output value
   2. Calculate gradients
      - ![image](https://user-images.githubusercontent.com/68107000/164116288-de1d93e0-a08e-4eee-b3fa-d1de8dfdc817.png)
   3. Update weights
      - ![image](https://user-images.githubusercontent.com/68107000/164116407-274dd604-1397-4db0-a711-3503e2f6c907.png)

## Matrix Notation

![image](https://user-images.githubusercontent.com/68107000/164113725-7c79979c-9bdf-4961-865f-bc7749ad1b1a.png)

x와 W는 내적과 같다.

## Batch Computing

앞서 행렬 연산을 왜 했을까? Batch Computing 을 도입하기 위해서이다.

![image](https://user-images.githubusercontent.com/68107000/164113202-4dacf1de-d8b0-4212-81f9-65398562f48c.png)

X와 W는 matrix matrix multiplication이다.

### 왜 Batch Computing을 도입하는가?i

- 사실, gradient는 전체 데이터셋에 대해서 하나를 구해야 한다.

- 컴퓨터 구조 상 벡터나 스칼라를 연산하는 것보다 행렬을 연산하는 것이 훨씬 빠르다.

## Epoch and Mini-Batch

하지만 현실에서는 전체 데이터셋에 대해서 하나를 구하는 것이 불가능하다.

- 만개, 10개의 데이터를 하나의 배치로 만들어 넣으면 오래걸리고 메모리에 안 올라간다.

그래서 training dataset을 mini-batch로 나누어 학습한다.

전체 데이터셋을 1번 학습 시키는 것을 1 epoch라고 한다.

## Hyperparameters

training 되지 않는, 사람이 정해주는 파라미터

- η, learning rate

- mini-batch size (실무에서 batch 로 통상적으로 쓰임)

- \# of epochs

training speed와 accuracy에 영향을 미치기에 잘 정하는 것이 중요하다.

![image](https://user-images.githubusercontent.com/68107000/164111753-021a2d25-559e-4ca5-975c-1348fe32c470.png)

learning rate가 작으면 시간이 오래 걸리고, 높으면 진동하거나 발산해서 절대 minimum point로 갈 수 없을 수 있다. 따라서 잘 고르는 것이 중요하다.
