# Neural Network Basics

## Modeling Pavlov Using a Neuron

ppt ê·¸ë¦¼ ë³´ê¸°, ì²«ë²ˆì§¸ ê·¸ë¦¼

![image](https://user-images.githubusercontent.com/68107000/164114686-ec20baad-8ca8-4076-b6ec-c7ac4a1ec10f.png)

## Mathematical Formulation of a Neuron

ë‰´ëŸ° í•˜ë‚˜ë§Œ ê°€ì§€ê³  ìˆëŠ” ëª¨ë¸ì„ **Perceptron** ì´ë¼ê³  í•œë‹¤.

![image](https://user-images.githubusercontent.com/68107000/163997735-530bafa4-6989-4c43-baf1-3be527558e6a.png)

## Gradient Descent

ìƒë¬¼ì€ STDP ë¡œ í•™ìŠµëë‹¤. ê¸°ê³„ì˜ weights ë“¤ì„ ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œ? â†’ Gradient Descentë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.

A famous optimization algorithm to **minimize a cost function** by iteratively moving in the direction of **steepest descent** as defined by the **negative gradient**

steepest descentì˜ ë°©í–¥ì€ negative gradientì˜ ë°©í–¥ì´ë‹¤.

<img src="https://user-images.githubusercontent.com/68107000/163999309-7dc65527-b8ad-4d6f-8730-cc6ed3b34744.png" title="" alt="image" data-align="center">ì˜ˆì‹œ: cost function Cë¥¼ ìµœì†Œí™”í•˜ëŠ” xë¥¼ ì°¾ì•„ë³´ì•˜ìŒ.

$$
argmin_x(C = x^2)
$$

initial pointëŠ” randomì´ë‹¤. 

## Training a Perceptron

Training a perceptron is solving an optimization problem with respect to weights.

errorë¥¼ ìµœì†Œí™”í•˜ëŠ” weights ë“¤ì„ ì°¾ì•„ì•¼ í•œë‹¤.

í•™ìŠµ ëª©í‘œëŠ” input from bellì˜ ì…ë ¥ì´ 1ì¼ ë•Œ outputì´ 1ì´ ë˜ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.

1. Initialize the weights to small random numbers
   - 0ìœ¼ë¡œ initial í•˜ë©´ ì•ˆë¨.
2. For each training sample ğ’™(ğ’Œ):
   - ê°ê°ì˜ ìƒˆë¡œìš´ training sampleì— ëŒ€í•´ì„œ ì§„í–‰í•œë‹¤.
   1. Calculate the output value
   2. Calculate gradients
      - ![image](https://user-images.githubusercontent.com/68107000/164116288-de1d93e0-a08e-4eee-b3fa-d1de8dfdc817.png)
   3. Update weights
      - ![image](https://user-images.githubusercontent.com/68107000/164116407-274dd604-1397-4db0-a711-3503e2f6c907.png)

## Matrix Notation

![image](https://user-images.githubusercontent.com/68107000/164113725-7c79979c-9bdf-4961-865f-bc7749ad1b1a.png)

xì™€ WëŠ” ë‚´ì ê³¼ ê°™ë‹¤.

## Batch Computing

ì•ì„œ í–‰ë ¬ ì—°ì‚°ì„ ì™œ í–ˆì„ê¹Œ? Batch Computing ì„ ë„ì…í•˜ê¸° ìœ„í•´ì„œì´ë‹¤.

![image](https://user-images.githubusercontent.com/68107000/164113202-4dacf1de-d8b0-4212-81f9-65398562f48c.png)

Xì™€ WëŠ” matrix matrix multiplicationì´ë‹¤.

### ì™œ Batch Computingì„ ë„ì…í•˜ëŠ”ê°€?i

- ì‚¬ì‹¤, gradientëŠ” ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ í•˜ë‚˜ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.

- ì»´í“¨í„° êµ¬ì¡° ìƒ ë²¡í„°ë‚˜ ìŠ¤ì¹¼ë¼ë¥¼ ì—°ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ í–‰ë ¬ì„ ì—°ì‚°í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë¹ ë¥´ë‹¤.

## Epoch and Mini-Batch

í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ í•˜ë‚˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.

- ë§Œê°œ, 10ê°œì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë§Œë“¤ì–´ ë„£ìœ¼ë©´ ì˜¤ë˜ê±¸ë¦¬ê³  ë©”ëª¨ë¦¬ì— ì•ˆ ì˜¬ë¼ê°„ë‹¤.

ê·¸ë˜ì„œ training datasetì„ mini-batchë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµí•œë‹¤.

ì „ì²´ ë°ì´í„°ì…‹ì„ 1ë²ˆ í•™ìŠµ ì‹œí‚¤ëŠ” ê²ƒì„ 1 epochë¼ê³  í•œë‹¤.

## Hyperparameters

training ë˜ì§€ ì•ŠëŠ”, ì‚¬ëŒì´ ì •í•´ì£¼ëŠ” íŒŒë¼ë¯¸í„°

- Î·, learning rate

- mini-batch size (ì‹¤ë¬´ì—ì„œ batch ë¡œ í†µìƒì ìœ¼ë¡œ ì“°ì„)

- \# of epochs

training speedì™€ accuracyì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸°ì— ì˜ ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

![image](https://user-images.githubusercontent.com/68107000/164111753-021a2d25-559e-4ca5-975c-1348fe32c470.png)

learning rateê°€ ì‘ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³ , ë†’ìœ¼ë©´ ì§„ë™í•˜ê±°ë‚˜ ë°œì‚°í•´ì„œ ì ˆëŒ€ minimum pointë¡œ ê°ˆ ìˆ˜ ì—†ì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì˜ ê³ ë¥´ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
