# Multi-Layer Perceptron

![image](https://user-images.githubusercontent.com/68107000/164123287-aed44a43-109d-4eb4-ba1b-788cb793417e.png)

뉴런은 7개이다.

층을 지칭할 땐, 시냅스 층을 말한다. 따라서 layer의 개수는 2개이다.

matrix-vector Mulitplication 연산이 필요하다.

2개의 activation function은 보통 다르다.

## 왜 Multi-Layer가 필요할까?: XOR 문제

A single perceptron cannot implement XOR.

XOR 문제는 linear separble한(선으로 나눌 수 있는) 문제가 아니기 때문이다.

<img src="https://user-images.githubusercontent.com/68107000/164126880-c4cacc8c-5f4e-47e2-a59f-0c30664d768c.png" title="" alt="image" data-align="center">

f가 없고 (동일하고) output을 0으로 가정하면 1차 함수 꼴(y = ax + b)이 되고 그래프로 나타내면 직선을 의미한다.

![image](https://user-images.githubusercontent.com/68107000/164127046-856aa5c6-5b40-4076-8b2f-41ad92389228.png)

하나의 뉴런이 하는 일은 input size가 n (위 하나의 뉴런에 n개의 선으로 연결된 input으로 들어옴)이면 n차원을 n-1차원의 hyperplane으로 나누는 것이다. 

input size가 2개이므로 2차원이고 2-1차원의 평면 즉 직선으로 나눌 수 있어야 한다.

하지만 위 그래프에서 어떻게 직선을 그려도 1과 0을 구분할 수 있는 직선은 그릴 수 없다.

## XOR: Intuition

(고정된 weight와 bias를 가진 2-layer perceptron으로 실제 계산해서 원하는 결과를 얻는 것을 보여주는 ppt가 있음. 올리지 않음.)

![image](https://user-images.githubusercontent.com/68107000/164128574-d05b904c-6173-437a-95cd-e60ae7f8a033.png)

첫번째 layer에서 feature extraction을 수행한다. 사람이 아니라 기계가 어떻게 space transformation (공간을 뒤틀어야, 변형해야) 구분할 수 있게 되는지 학습하게 된다. (그래서 딥러닝이 강력한 것이다!)

두번째 layer에서는 공간 상에서 linear separable하게 최종적으로 가르는 역할을 한다. 분류한다고 해서 classification이라고 한다.

**정리**

> perceptron에서 하나의 뉴런이 하는 일은 input size가 n일 때 n차원을 n-1차원의 hyperplane으로 나누는 것이다.
> XOR 문제는 input size가 2개이므로 2차원으로 표현할 수 있고 2-1차원의 평면 즉 직선으로 나눌 수 있어야 한다.
> 하지만 2차원 좌표평면에서 (0,0), (1,1)은 0의 값을 가지면서 (0,1), (1,0)은 1의 값을 표시해두었을 때 어떠한 직선을 그려도 0과 1을 구분할 수 없다.
> XOR 문제는 linear separable한 문제가 아니다.
> 0과 1을 구분할 수 있도록 공간을 변형시키는 것이 필요하다.
> 층을 쌓아 non-linearty를 부여할 수 있는 activation function을 적용해 space transformation을 해야한다.
> 이 과정을 feature extraction이라고 한다.
> 따라서 feature extraction을 위한 레이어가 더 필요하기 때문에 multi-layer가 필요하다.

## MLP: Training

어떻게 weight와 bias를 학습할까?

>  Training an MLP is finding weight/bias matrices given training data to achieve our goal.
> 
> ![image](https://user-images.githubusercontent.com/68107000/164129530-b8da48ee-23bf-4b2b-b61a-dffcf4b2ff01.png)

하지만 이 문제는 analytically하게 풀 수 없다!

그래서 gradient descent를 사용한다.

## Gradient Descent: Perceptron

![image](https://user-images.githubusercontent.com/68107000/164200666-24ec4ca5-2938-4bef-be86-32e910566e68.png)

<img src="https://user-images.githubusercontent.com/68107000/164200710-84103b92-a522-4c2f-b22b-1addc6b36f6f.png" title="" alt="image" data-align="center">

## Gradient Descent: MLP

We need to compute **partial derivatives** of the cost function with respect to weights

<img src="https://user-images.githubusercontent.com/68107000/164129769-4ebb282f-413c-4691-8339-9306fc89a3e0.png" title="" alt="image" data-align="center">

**output 뉴런이 1개인 경우:**

![image](https://user-images.githubusercontent.com/68107000/164204178-01caac3d-e406-4825-a958-d4ee66f8b4a0.png) 

**output 뉴런이 2개인 경우:**

![image](https://user-images.githubusercontent.com/68107000/164204719-8c5612b0-94ef-4923-b6d5-aa716c5e6a27.png)

## Training an NN: Backpropagation Algorithm

1. Initialize the weights/biases to small random numbers
2. Across an entire of training dataset:
   1. **Forward propagation**: calculate the output value, and 
      compute error for the entire dataset
   2. **Backpropagate** errors through network
   3. **Update weights/biases**
3. Repeat until network is trained well

이 부분은 계산 문제로 시험에 거의 백프로 나온다. 가장 어려운 계산 과정 부분만 올려두었다.

<img title="" src="https://user-images.githubusercontent.com/68107000/164131953-ccc30634-3772-434c-a47f-6cf657f007a5.png" alt="image" data-align="center">
