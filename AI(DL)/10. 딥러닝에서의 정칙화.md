# Regularization

딥러닝이라는 분야 전체를 관통하는 중요한 개념

## Cross-entropy loss function

- 이 전까지는 loss/cost/error function으로 MSE 를 사용했지만 mean squared error는 굉장히 단순하고 실제로는 극히 드물게 사용된다.

- cross-entropy loss function은 classfication에서 널리 쓰인다.

- 미분 가능 함수이다.

- ![image](https://user-images.githubusercontent.com/68107000/164947528-f0a2d3d2-9ac1-4cf9-b189-3f88e06c51ae.png)

- t: ground truth, o: NN output, c: \# of class

- t와 o의 거리를 측정한다. 다르면 값이 커진다.

- CE의 값이 커진다 network의 예측이 불확실하다.

- 수학적으로 G.T와의 차이가 1에 가까울수록 error(loss)가 크다. (0에 가까울수록 작다.)

- 사용은 2가지 방법이 있음. BCE, CCE

### entropy란?

- 확률 변수 불확실성

- ![image](https://user-images.githubusercontent.com/68107000/164947468-5b08c570-a4d4-43f3-923c-55448cb11f4f.png)

- X: 확률변수, p(x): X에 대한 확률분포, 0 < p(x) < 1, log(p(x)) < 0이라서 음수를 붙여 양수로 환원

- p(x)의 불확실성이 커질수록 entropy가 커진다.

- ![image](https://user-images.githubusercontent.com/68107000/164947870-94eb09c2-6cff-4416-8ba5-bb69ce1ade3c.png)

- 3에서는 O가 거의 확실하다. entropy값은 0.2108이다.

- 1에서는 ▷가 꽤 확실하다. entropy값은 0.5665이다.

- 2에서는 무엇을 뽑게 될지 불확실하다. entropy값은 0.9968이다.

### binary classification

- binary cross-entropy loss

- ![image](https://user-images.githubusercontent.com/68107000/164947408-0e3a40f8-0eeb-498e-82a9-054830b5dff8.png)

- after sigmoid function

### multi-class classification

- categorical cross-entropy loss

- ![image](https://user-images.githubusercontent.com/68107000/164947414-b7642c6b-11ca-4c04-ae86-3786dc72d79e.png)

- after softmax

## Regularization

- The process of adding information(데이터) in order to **prevent overfitting**
  
  - overfitting은 일반화가 되지 않고 특수한 상황에서만 training되는 것을 말함.
  - 일반적인 상황에서도 잘 동작하도록 하는 것이 AI의 목표

- A common pattern: **add** some kind of **randomness** at training
  
  - random한 noise를 포함해서 학습해서 일반화된 데이터에 잘 대응할 수 있게 됨.

## Underfitting/Overfitting

![image](https://user-images.githubusercontent.com/68107000/164883889-900af749-5f5e-42df-a329-901d85977668.png)

- 같이 떨어질 때 underfitting

- Overfitting results in very low training error. However, it gives high variance. 즉, validation error much higher than training error. 격차가 큼.

## Validation set

- **Validation set** is used to optimize the model parameters (lr, epochs, hidden layer unit 수, etc)
  
  - validation set으로 parameter를 수정해 model을 validation 쪽으로 편향시킨다.

- while the **test set** is used to provide an unbiased estimate of the final model
  
  - bias 전혀 안된 test set으로 test 해야 일반화가 잘 되었는지로 제대로 된 성능을 뽑을 수 있다.
  - validation set을 통해 결정된 최종 모델의 성능 측정할 때 사용한다.

- validation set을 포함한 학습 방법

![image](https://user-images.githubusercontent.com/68107000/164883827-017675aa-20de-4128-8c8f-f8745be2b0a2.png)

- 따라서 반드시 어떤 학습을 수행할 때, test loss, validation loss, training loss 그래프를 그려서 overfitting이 생겼는지 확인해야한다.
  
  - 무작정 training loss가 낮다고 학습이 잘 된 것이 아니다.
  
  - validation loss와 training loss의 차이가 크다면 overfitting이 일어난 것이다.

## Model architecture selection

- Using too complex model can result in overfitting

- 예시를 들어, XOR 문제는 푸는데 너무 복잡한 모델 (100 layer)을 사용하면 overffiting이 일어나게 됨.

## Larger Dataset Size

- Larger dataset prevents over-fitting

- For the same model complexity, 데이터가 많을수록 잘 학습됨.

## Data augmentation (데이터 증강)

하지만 데이터셋이 적다면? 데이터 증강을 해보아야 한다.

- horizontal flip

- random crops

- color jitter (조금씩 움직이다)

## Weight decay

- overfitting 시 weight 값이 커지는 경향이 있어서, weight가 small value를 가질 수 있도록 억제하는 방법이다. weight에 penalty를 준다.

- 실제로 error function에 L1 또는 L2 regularization term을 더한다.
  
  - ![image](https://user-images.githubusercontent.com/68107000/164883325-d8f77ec5-7fa6-42ba-af63-e305f0c15617.png)
  
  - 2번째 term이 실제 L2 function, weight 크기에 대한 panelty
  
  - w 커지면 L 커진다. 그런데 training을 L ↓ 방향으로 하기에 L↓ → weight↓ 쪽으로 간다.

λ: weight decay에 영향력을 조절할 수 있는 hyper parameter

![image](https://user-images.githubusercontent.com/68107000/164883172-db13ba51-c46d-4845-bb00-da788e61c957.png)

λ이 너무 크면 weight decay ↑, weight size가 거의 0에 가까워 학습이 거의 안이뤄진다.

λ이 너무 작으면 weight decay  거의 안함. 과적합하는 그 상태 그대로.

## Early stopping

![image](https://user-images.githubusercontent.com/68107000/164883152-3a3440aa-a929-472a-bd75-a738e6f1b730.png)

overfitting을 방지한다기보다는 피하는 방법

underfitting에서 overfitting으로 넘어가려고 할 때까지만, 덜 학습시킨 모델을 가져옴.

## Dropout

- A regularization method

- At training: drop connections between neurons(뉴런과의 연결 = 시냅스) randomly
  
  - Dropout with probability p removes p portion of connections
  
  - E.g., dropout 0.5: drop half(50%) of connections.
  
  - training할 때마다 random하게 50%씩 없앰.

- At test: use all connections (전결합)

- 지워진 connection 없이도 잘 training 할 수 있도록 유도하여 더욱 일반화하는 효과를 가진다.

- noise를 inject하는 효과가 있다. 학습이 더 잘 된다.

## Batch normalization

- 이미지의 input feature or activation들의 mean과 variance를 어떤 특정한 값으로 바꿔주는(조정해주는) 것.

- 이미지값을 바꿔주면 regularization 효과를 불러 일으켜서 training이 더 잘된다는 연구가 있다.

- ![](C:\Users\Jueun\AppData\Roaming\marktext\images\2022-04-24-07-38-37-image.png)

- mean을 바꾼다는 것은 전체 pixel 값을 μB 만큼 shift한다는 뜻이고 variance를 바꾼다는 것은 σB^2만큼 scaling해준다는 뜻이다.

- CNN에서 특히 쓰임.

![image](https://user-images.githubusercontent.com/68107000/164883514-caf412e4-6340-41ec-8043-2ce50337bfed.png)

- 평균과 분산은 주어진 모델을 훈련시키는데 사용된 이전의 입력 데이터에 따라 선택된다.

- 매 layer마다 μB와 σB^2이 있는데 자체를 트레이닝해서 얻는다.

- 어떤 방향으로 이미지를 shift하고 scaling되어야 가장 적합한 normalization이 되는지를 기계가 찾는다.
