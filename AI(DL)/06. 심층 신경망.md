# Deep Neural Network

## Universal Approximation Theorem

MLP 하나만으로 어떠한 함수도 근사할 수 있다.

여기서 의미하는 MLP는 input layer와 hidden layer 로 2개의 층을 가지고 있는 모델을 말하는 것이다.

hidden layer에 충분한 뉴런 개수가 제공되고 적절한 weights라면 말이다.

## 왜 Acitivation Function이 중요한가?

activation function으로는 non-linear function을 사용한다.

참고로 linear하다는 것은 y = f(x) = ax + b 꼴로 표현이 된다는 것을 의미한다.

non-linear function은 non-linearity를 부가해서 non-linear한 문제를 해결할 수 있게 만들어 준다.

activation function을 없애면 XOR 문제를 풀기 위해서 층을 쌓았던 것이 무의미해진다. 층을 쌓아도 또다른 linear classifier 가 도출될 뿐이다. XOR 문제는 linear separable하지 않게 때문에 linear한 식으로는 풀 수가 없다.

## Activation Function

![image](https://user-images.githubusercontent.com/68107000/164355777-61aa722a-3ddc-47c6-8871-68f2a027606c.png)

ReLU 함수를 많이 사용한다.

## Derivative Activation Function

Gradient Descent에서는 activation func의 미분 값을 계산해야 한다.

따라서 미분이 가능하고 non-linear한 함수가 쓰인다.

![image](https://user-images.githubusercontent.com/68107000/164355814-9cf62d37-1f3c-468d-9fbb-efc3c41e16ef.png)

## Deep Neural Network

**Perceptron**: 1개의 뉴런 (1개의 input layer)

**MLP**: 2-Layer Perceptron (2개의 시냅스 층)

**Deep Neural Network**: NN with **<u>*deep*</u>** layers.

**DNN의 종류**

- n-Layer Perceptron (보통 n ≥ 5, or Fully-Connected Network or Dense Layer in Tensorflow)

- n-Layer Convolutional Neural Network 

- Recurrent Neural Network

- 이들끼리 섞을 수도 있음. 예를 덜어 CONV Layer + FC Layer

## 왜 깊어져야할까?: Circuit Theory Analogy

깊어질수록 추상화가 low level에서 high level로 진행된다.

**small** n-layer **deep** NN = **Shallower** network with **exponentially more hidden units**

universal approximation theorem에 의하면 2 layer만으로도 이론적으로 근사가 가능하지만 **효율이 최악**이다. 쉽게 예를 들어 여러 층에 걸쳐 100개의 뉴런을 사용한 DNN은 2 layer로 구현하기 위해서 2^100개의 뉴런이 필요할 수도 있다.

이를 논리회로에 비유할 수 있다. 8자리의 parity checker를 (0, 1을 나타낼 수 있는 8개의 비트가 있는데, 1의 개수가 짝수면 0, 홀수면 1을 output으로 내는) 논리회로를 만든다고 해보자.

![image](https://user-images.githubusercontent.com/68107000/164358400-55340ae3-1548-4bc2-a621-2b5036ce6068.png)

왼쪽은 8개의 input에 대해서 3개의 층으로 7개의 xor gate를 사용했다. 일반화하면 n개의 input일 때 n-1개의 XOR gate가 필요하다고 할 수 있다.

오른쪽은 2^8개의 각 경우의 수를 하나의 AND gate에 대응시키고 있다. 마지막으로 OR gate를 추가했다. 일반화하면 2^n개의 AND gate가 필요하다고 할 수 있다.

## Training DNN

(skip) > ppt에 상세히 적어둠.

말로 정리해보면

- weight update를 위한 **dW(L)**
  
  - dW(L)는 dz(L) 내적 원래의 input y(L-1)의 T 로 수행하면 된다.
  
  - dz(L)는 뒤에서 backpropagate된 error (input gradient)인 dy(L) element wise f'(z(L)) 인데 z(L)은 activation func을 거치지 않고 weighted sum만 한 값

- (다시 말하지만) 뒤에서 backpropagate된 error (input gradient) **dy(L)**
  
  - 만약 맨 마지막 layer라면 forward에서 구한 output과의 cost로 미분한 loss func 을 거친 값이고 `loss_deactivation_func(O, T)`
  
  - 맨 마지막이 아니라면 W(L+1) 곱하기 dZ(L) 인 값을 사용하면 된다. ``

## Hyperparameters

Parameters: 기계가 학습

- Weight

- bias

Hyperparameters: 사람이 지정

- Learning rate (η)

- \# of training iterations (= epochs)

- batch size

- \# of hidden layers

- \# of hidden units

- \# of hidden functions

- \# of hidden types

아래 4개는 네트워크 아키텍쳐 혹은 모델 아키텍쳐에 해당한다.

## Random Initialization

weight를 잘 초기화해주는 것이 중요하다. 보통 small random number로 초기화해준다.

cost function에서 initial point가 다르면 다른 학습 결과가 도출될 수 있다. (울렁거리는 곡면에서 구슬을 어디에 떨어뜨리냐에 따라 다른 구멍에 들어가듯이)

만약 0으로 초기화한다면?

tahn, ReLU: GT의 값에 상관없이 wieght는 update 해도 0을 유지한다.

sigmoid: 각 층에서 같은 input에서 나가는 weight들이 묶여 같은 값으로 변화한다. sub optimal하다. 따라서 원하는 솔루션에 도달할 수 없다.

## Application: Face Recognition

### Input encoding

- extract key feature using preprocessing (human-designed)
  
  - 사람이 직접 고민한 특징을 뽑아 input으로 넣어준다.
  
  - 예를 들면, edges, regions of uniform intensity 등
  
  - 단점: high preprocessing cost (시간 오래 걸림), variable number of features (feature 개수가 이미지마다 다르다)

- Coarse-resolution (Why?)
  
  - 복잡도를 줄여 학습을 더 잘함.
  
  - 원래 128x120 pixel image를 → 32x30으로 coarse resolution summary 줄여서 넣어준다. (16배 줄어들었다.)

### Output encoding

- one unit scheme
  
  - 하나의 output을 사용하고 여러 threshold values 를 둔다.
  
  - 즉 여러 급간으로 나눈다.

- multiple unit scheme (**one-hot encoding**)
  
  - 4개의 구별된 output 유닛
  
  - 각 유닛은 해당 방향일 확률을 나타내므로, 가장 높은 값의 output이 네트워크의 예측이 된다.
  
  - 장점
    
    - 네트워크에서 target function을 나타내는데 자유도가 높다.
    
    - 첫번째 큰 값과 두번째 큰 값의 차이로 네트워크 예측의 confidence를 측정할 수 있다.
  
  - \<1, 0, 0, 0\> vs. <0.9, 0.1, 0.1, 0.1>
    
    - 전자는 bound 없이 계속 증가한다. 최종 output activation function이 sigmoid일 경우, output이 1.0이 될 수 없는데, 한없이 0.9xxx로 다가갈 것이다.
    
    - 후자는 finite weights를 가지게 될 것이다.

### Network Architecture

- one hidden layer vs. more hidden layers

- 몇 개의 hidden node를 사용해야할까?
  
  - 3 hidden units
    
    - test acc for face date = 90%
    
    - training time = 5분
  
  - 30 hidden units
    
    - test acc for face date = 91.5%
    
    - training time = 1시간

뭐가 더 좋은지는 우선순위에 따라 정할 수 있다.

## Hyperparameters

trial error로 많이 결정한다.

- learning rate η = 0.3

- weight initialization: 0 근처의 작은 랜덤 값

- \# of epochs
  
  - cross validation에 의해 결정됨.
  
  - 각 epoch마다 validation set (training set과 다른 데이터셋)으로 네트워크 성능을 측정한다.
  
  - validation set에 의해 가장 높은 accuracy를 보인 것을 최종 네트워크로 선택한다.
